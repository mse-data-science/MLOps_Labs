{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01c: Transformers - Solution\n",
    "\n",
    "Transformers probably don't need an introduction, but in case you need a refresher: The transformer neural network architecture is what powers the likes of GPT, Llama, and co. In this notebook, you will implement GPT from scratch. Yes, you read that right.\n",
    "\n",
    "Now that I have your _attention_, let's look at what we have to understand and implement to achieve this feat.\n",
    "\n",
    "First and foremost, transformers are language models. They are trained on raw text (or any other data modality) in a self-supervised fashion. The objective is computed directly from the data, no human labeling is needed.\n",
    "\n",
    "The transformer architecture comprises two building blocks:\n",
    "- An encoder that receives the input and builds features from it.\n",
    "- A decoder that uses these features (along with other inputs) to generate an output sequence (usually probabilities).\n",
    "\n",
    "Depending on the task, a model might have both or only one of the two.\n",
    "\n",
    "Encoder-Decoder architectures are not new. Autoencoders can do that too. What makes transformers special is _attention_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Before we worry about attention, we first have to transform words (or sentences) into a form that a neural network can understand. This is achieved by embedding the words. Let's look at the sentence: \"Life is too short for bad coffee.\"\n",
    "We restrict our vocabulary to the words that occur in this sentence, in reality the vocabulary is of course **much** bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42) # for reproducibility\n",
    "\n",
    "sentence = \"Life is too short for bad coffee\"\n",
    "\n",
    "vocab = {s:i for i,s in enumerate(sorted(sentence.split()))}\n",
    "n = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our vocabulary, we can assign an integer index to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sentence_int = torch.tensor([vocab[s] for s in sentence.split()])\n",
    "sentence_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), we can transform the integer representation `sentence_int` into a real-valued embedding. The embedding layer is simply a look-up table for embeddings of a fixed dictionary (the vocab) and size. We'll use 8-dimensional embeddings (for future reference, $d = 8$). Together with the $n = 7$ words in the vocab, we arrive at a 7x8-dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.Embedding(len(vocab), 8)\n",
    "embedded = embedding(sentence_int).detach()\n",
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have ever used the [ðŸ¤— Transformers library](https://huggingface.co/docs/transformers/index) you have very likely used some type of `Tokenizer`. Tokenizers implement the process described above - although on a much larger scale. When you use a tokenizer to process a piece of text, it breaks it down into tokens and assigns a unique numerical identifier (token ID) to each token. These token IDs are what the model uses as input during training or inference. The model's embedding layer then looks up the corresponding embeddings for these token IDs from its embedding matrix.\n",
    "\n",
    "## Attention\n",
    "\n",
    "The key feature of Transformer models is their use of attention, more precisely _self-attention_. Self-Attention was introduced in the publication [Attention is all you need](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "Attention mimics human cognitive attention by calculating \"soft\" weights for each word in the current context window. Soft weights can change at runtime, as opposed to \"hard\" weights which are computed through, and constant after, training.\n",
    "\n",
    "The self-attention mechanism is also known as _scaled dot-product attention_ which will make sense, once you see the mathematical formula describing self-attention. \n",
    "\n",
    "The trainable components of self-attention are the three weight matrices, $\\mathbf{W}_q, \\mathbf{W}_k, \\mathbf{W}_v$. They project the input sequence $\\mathbf{x}$ to form _query_ ($q$), _key_ ($k$), and  _value_ ($v$) sequences:\n",
    "\n",
    "- query sequence $\\mathbf{q}^{(i)}=\\mathbf{W}_q \\mathbf{x}^{(i)}$\n",
    "- key sequence $\\mathbf{k}^{(i)}=\\mathbf{W}_k \\mathbf{x}^{(i)}$ \n",
    "- value sequence $\\mathbf{v}^{(i)}=\\mathbf{W}_v \\mathbf{x}^{(i)}$\n",
    "\n",
    "where $i \\in [1, T]$ is the token index and $T$ the length of the input sequence. Both $q^{(i)}$ and $k^{(i)}$ are vectors of dimension $d_k$. This is important, because $q$ and $k$ will later be \"dot-producted\" together. The projection matrices $\\mathbf{W}_q$ and $\\mathbf{W}_k$ are $d_k \\times d$, while $W_v$ is $d_v\\times d$. $d_v$ is not constrained by the dimensions of the other vectors as it is the size of the resulting context vector. \n",
    "\n",
    "For now, we'll set $d_q = d_k = 24$ and $d_v = 30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = embedded.shape[1]\n",
    "\n",
    "#Â Solution: Set the dimensions of the query, key and value vectors\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_q = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "#Â Solution: Create the key and value weight matrices\n",
    "W_k = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_v = torch.nn.Parameter(torch.rand(d_v, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "### Unnormalized attention weights\n",
    "\n",
    "Computing the unnormalized attention weight that the $i$-th word (\"the query\") attributes to the $j$-th word (\"the key\") is straight forward: it's simply the _dot-product_ of the corresponding query and key vectors:\n",
    "\n",
    "$$\n",
    "\\omega_{i,j} = {\\mathbf{q}^{(i)}}^\\top \\mathbf{k}^{(j)}\n",
    "$$\n",
    "\n",
    "Let's pick the fourth word (zero-indexed, so $i = 3$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_4 = embedded[3]\n",
    "q_4 = W_q.matmul(x_4)\n",
    "k_4 = W_k.matmul(x_4)\n",
    "v_4 = W_v.matmul(x_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn, generalize the computation of the keys and values to all $j$. \n",
    "\n",
    "_Hint: It involves matrix multiplication._ Ensure that $K$ is $d_k \\times n$ and $V$ is $d_v \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Exercise: Implement this.\n",
    "K = W_k.matmul(embedded.T)\n",
    "V = W_v.matmul(embedded.T)\n",
    "\n",
    "assert K.shape == (d_k, len(sentence.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $K$ available, computing the unnormalized weight vector for the $i$-th token is simple as simple as multiplying the query vector with the $K$-matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_4 = q_4.matmul(K)\n",
    "omega_4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention scores\n",
    "\n",
    "Attention scores $\\alpha_{i, j}$ are simply normalized attention weights $\\omega_{i, j}$ passed through a softmax:\n",
    "\n",
    "$$\\alpha_{i, j} = \\operatorname{softmax}(\\frac{\\omega_{i, j}}{\\sqrt{d_k}})$$\n",
    "\n",
    "Scaling by $d_k$ ensures that the Euclidean norm of the weight vectors will be approximately of the same magnitude. This aids in curbing numerical instabilities during training.\n",
    "\n",
    "Compute the attention scores for the forth word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Exercise: Compute alpha_4\n",
    "alpha_4 = F.softmax(omega_4 / d_k ** 0.5, dim=0).detach()\n",
    "alpha_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context vectors\n",
    "\n",
    "The final step in the attention mechanism is the computation of the context vector. This is simply the input vector but re-weighted by the attention scores:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{i} = \\sum\\limits_{j = 1}^{T}\\alpha_{i, j} \\mathbf{v}^{(j)}\n",
    "$$\n",
    "\n",
    "Below, compute the context vector for the forth word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Now that we've seen the self-attention mechanism in detail, let's implement an attention layer. Attention layers in $torch$ and similar libraries don't implement the computation of the query, key, and value vector but instead leave this to preceding layers in the neural network. As such, you only have to implement the $\\operatorname{softmax}(\\dots)$-part and return the context vector.\n",
    "\n",
    "We've already provided you with a template in the cell below. Note that `query` is a matrix of $\\mathbf{q}$-vectors, i.e. the matrix product of $W_q$ and the input embedding matrix.\n",
    "\n",
    "_If you are looking for inspiration, the [PyTorch documentation for `scaled_dot_product_attention`](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#) might be helpful._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Compute the dot product of query and key\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "        # Scale the scores by the square root of the key dimension\n",
    "        scores = scores / torch.sqrt(torch.tensor(key.size(-1), dtype=torch.float32))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Compute the weighted sum of the value vectors\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "One set of weight matrices $(\\mathbf{W}_q, \\mathbf{W}_k, \\mathbf{W}_v)$ is called an _attention head_. Each layer in a transformer actually has multiple heads. Since one attention head learns some notion of relevance, multiple attention heads allow the model to use multiple notions of relevance simultaneously. Additionally, the influence field representing \"relevance\" becomes narrower with increasing depth.\n",
    "\n",
    "Interestingly, the different heads tend to learn concepts that are meaningful to humans: Some heads may attend to the next word, while others attend to the direct or indirect object in a sentence.\n",
    "\n",
    "So, if multiple heads are so [powerful](https://arxiv.org/abs/2104.00887) (which, by the way, is somewhat [debatable](https://arxiv.org/abs/1905.10650)), how do we implement them?\n",
    "It's sadly a bit underwhelming: Multi-head attention is simply multiple heads concatenated together:\n",
    "\n",
    "$$\n",
    "\\operatorname{MultiHead(Q, K, V)} = \\operatorname{Concat}(\\operatorname{head}_1, \\dots, \\operatorname{head}_n)\\mathbf{W}^O\n",
    "$$\n",
    "\n",
    "where $\\operatorname{head}_i = \\operatorname{Attention}(Q, K, V)$. Implementation-wise, not much changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of heads\n",
    "headcnt = 4\n",
    "\n",
    "# The heads are packed in a single dimension. We could also use a 3D tensor \n",
    "#Â but that would be more complicated.\n",
    "W_q = torch.randn(headcnt * d_q, d) / (d**0.5)\n",
    "W_k = torch.randn(headcnt * d_k, d) / (d**0.5)\n",
    "W_v = torch.randn(headcnt * d_v, d) / (d**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{K}, \\mathbf{V}$ computation is the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = W_k.matmul(embedded.T)\n",
    "V = W_v.matmul(embedded.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the actual attention computation also proceeds as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_4 = W_q.matmul(x_4)\n",
    "omega_4 = q_4.matmul(K)\n",
    "alpha_4 = F.softmax(omega_4 / d_k ** 0.5, dim=0).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal attention mask\n",
    "\n",
    "The final ingredient is masking: For models such as GPT, each token can only attend to tokens before it, thus the attention score needs to be modified before entering softmax.\n",
    "The most common way of masking is to add a large negative number to the locations that you'd not want the model to attend to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.ones(n, n)\n",
    "attn_mask = -1E4 * torch.triu(attn_mask,1)\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a mask, you obtain masked attention by simply adding it to your normalized weights:\n",
    "\n",
    "$$\n",
    "\\operatorname{MaskedAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\operatorname{softmake}(M + \\frac{\\mathbf{QK}}{\\sqrt{d_k}})\n",
    "$$\n",
    "\n",
    "where $M$ is the mask.\n",
    "\n",
    "## Transformer Block\n",
    "\n",
    "Having gained some intuition for attention, we can now begin to assemble our transformer. First, we need `MaskedMultiHeadAttention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super(MaskedMultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Hey, look, it's your friend!\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Masked attention\n",
    "        mask = torch.ones(batch_size, 1, 1, query.size(1))\n",
    "        mask = -1e4 * torch.triu(mask, 1)\n",
    "        output, attention_weights = self.attention(Q, K, V, mask)\n",
    "\n",
    "        # Reshape and concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see a schematic of GPT2. The block that is repeated twelve times is a transformer block.\n",
    "\n",
    "![GPT2 Transformer Block](imgs/gpt_transformer_block.png)\n",
    "\n",
    "We have prepared a skeleton for your GPT2 transformer block. Implement the block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GPT2TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(GPT2TransformerBlock, self).__init__()\n",
    "\n",
    "        # Exercise: Your layer definitions here\n",
    "        self.self_attention = MaskedMultiHeadAttention(\n",
    "            d_model, num_heads, dropout=dropout\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Don't change this, we already implemented it for you\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Exercise: Self-attention\n",
    "        attn_output, _ = self.self_attention(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # Exercise: Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task\n",
    "\n",
    "You have implemented all the components required for GPT2 - but you can go further.\n",
    "Here's how:\n",
    "\n",
    "- Use the `transformers` library to download weights for GPT2.\n",
    "- Construct a GPT2 model using your blocks.\n",
    "- Copy the downloaded GPT2 weights into your own own implementation.\n",
    "- Congrats, you now have a working GPT2 :) (minus the output layer... ðŸ¤«)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-lab-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
