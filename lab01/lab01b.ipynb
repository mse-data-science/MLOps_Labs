{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01b: Convolutional Neural Networks\n",
    "\n",
    "## Why convolutions?\n",
    "###### This part of the lab is based on the [2022 edition of Full Stack Deep Learning Lecture](https://fullstackdeeplearning.com/course/2022/).\n",
    "\n",
    "The most basic neural networks,\n",
    "multi-layer perceptrons,\n",
    "are built by alternating\n",
    "parameterized linear transformations\n",
    "with non-linear transformations.\n",
    "\n",
    "This combination is capable of expressing\n",
    "[functions of arbitrary complexity](http://neuralnetworksanddeeplearning.com/chap4.html),\n",
    "so long as those functions\n",
    "take in fixed-size arrays and return fixed-size arrays.\n",
    "\n",
    "```python\n",
    "def any_function_you_can_imagine(x: torch.Tensor[\"A\"]) -> torch.Tensor[\"B\"]:\n",
    "    return some_mlp_that_might_be_impractically_huge(x)\n",
    "```\n",
    "\n",
    "But not all functions have that type signature.\n",
    "\n",
    "For example, we might want to identify the content of images\n",
    "that have different sizes.\n",
    "Without gross hacks,\n",
    "an MLP won't be able to solve this problem,\n",
    "even though it seems simple enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import IPython.display as display\n",
    "\n",
    "randsize = 10 ** (random.random() * 2 + 1)\n",
    "\n",
    "Url = \"imgs/U.png\"\n",
    "\n",
    "# run multiple times to display the same image at different sizes\n",
    "#  the content of the image remains unambiguous\n",
    "display.Image(url=Url, width=randsize, height=randsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even worse, MLPs are too general to be efficient.\n",
    "\n",
    "Each layer applies an unstructured matrix to its inputs.\n",
    "But most of the data we might want to apply them to is highly structured,\n",
    "and taking advantage of that structure can make our models more efficient.\n",
    "\n",
    "It may seem appealing to use an unstructured model:\n",
    "it can in principle learn any function.\n",
    "But\n",
    "[most functions are monstrous outrages against common sense](https://en.wikipedia.org/wiki/Weierstrass_function#Density_of_nowhere-differentiable_functions).\n",
    "It is useful to encode some of our assumptions\n",
    "about the kinds of functions we might want to learn\n",
    "from our data into our model's architecture.\n",
    "\n",
    "## Convolutions are the local, translation-equivariant linear transforms.\n",
    "\n",
    "One of the most common types of structure in data is \"locality\" --\n",
    "the most relevant information for understanding or predicting a pixel\n",
    "is a small number of pixels around it.\n",
    "\n",
    "Locality is a fundamental feature of the physical world,\n",
    "so it shows up in data drawn from physical observations,\n",
    "like photographs and audio recordings.\n",
    "\n",
    "Locality means most meaningful linear transformations of our input\n",
    "only have large weights in a small number of entries that are close to one another,\n",
    "rather than having equally large weights in all entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "generic_linear_transform = torch.randn(8, 1)\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "local_linear_transform = torch.tensor(\n",
    "    [[0, 0, 0] + [random.random(), random.random(), random.random()] + [0, 0]]\n",
    ").T\n",
    "print(\"local:\", local_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of structure commonly observed is \"translation equivariance\" --\n",
    "the top-left pixel position is not, in itself, meaningfully different\n",
    "from the bottom-right position\n",
    "or a position in the middle of the image.\n",
    "Relative relationships matter more than absolute relationships.\n",
    "\n",
    "Translation equivariance arises in images because there is generally no privileged\n",
    "vantage point for taking the image.\n",
    "We could just as easily have taken the image while standing a few feet to the left or right,\n",
    "and all of its contents would shift along with our change in perspective.\n",
    "\n",
    "Translation equivariance means that a linear transformation that is meaningful at one position\n",
    "in our input is likely to be meaningful at all other points.\n",
    "We can learn something about a linear transformation from a datapoint where it is useful\n",
    "in the bottom-left and then apply it to another datapoint where it's useful in the top-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_linear_transform = torch.arange(8)[:, None]\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "equivariant_linear_transform = torch.stack(\n",
    "    [torch.roll(generic_linear_transform[:, 0], ii) for ii in range(8)], dim=1\n",
    ")\n",
    "print(\"translation invariant:\", equivariant_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear transformation that is translation equivariant\n",
    "[is called a _convolution_](https://en.wikipedia.org/wiki/Convolution#Translational_equivariance).\n",
    "\n",
    "If the weights of that linear transformation are mostly zero\n",
    "except for a few that are close to one another,\n",
    "that convolution is said to have a _kernel_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the equivalent of torch.nn.Linear, but for a 1-dimensional convolution\n",
    "conv_layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "\n",
    "conv_layer.weight  # aka kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using normal matrix multiplication to apply the kernel to the input,\n",
    "we repeatedly apply that kernel over and over again,\n",
    "\"sliding\" it over the input to produce an output.\n",
    "\n",
    "Every convolution kernel has an equivalent matrix form,\n",
    "which can be matrix multiplied with the input to create the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <small> Under the hood, the actual operation that implements the application of a convolutional kernel\n",
    "need not look like either of these\n",
    "(common approaches include\n",
    "[Winograd-type algorithms](https://arxiv.org/abs/1509.09308)\n",
    "and [Fast Fourier Transform-based algorithms](https://arxiv.org/abs/1312.5851)).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though they may seem somewhat arbitrary and technical,\n",
    "convolutions are actually a deep and fundamental piece of mathematics and computer science.\n",
    "Fundamental as in\n",
    "[closely related to the multiplication algorithm we learn as children](https://charlesfrye.github.io/math/2019/02/20/multiplication-convoluted-part-one.html)\n",
    "and deep as in\n",
    "[closely related to the Fourier transform](https://math.stackexchange.com/questions/918345/fourier-transform-as-diagonalization-of-convolution).\n",
    "Generalized convolutions can show up\n",
    "wherever there is some kind of \"sum\" over some kind of \"paths\",\n",
    "as is common in dynamic programming.\n",
    "\n",
    "In the context of this course,\n",
    "we don't have time to dive much deeper on convolutions or convolutional neural networks.\n",
    "\n",
    "See Chris Olah's blog series\n",
    "([1](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/),\n",
    "[2](https://colah.github.io/posts/2014-07-Understanding-Convolutions/),\n",
    "[3](https://colah.github.io/posts/2014-12-Groups-Convolution/))\n",
    "for a friendly introduction to the mathematical view of convolution.\n",
    "\n",
    "For more on convolutional neural network architectures, see\n",
    "[the lecture notes from Stanford's 2020 \"Deep Learning for Computer Vision\" course](https://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "Images have two dimensions of translation equivariance:\n",
    "left/right and up/down.\n",
    "So we use two-dimensional convolutions,\n",
    "instantiated in `torch.nn` as `nn.Conv2d` layers.\n",
    "Note that convolutional neural networks for images\n",
    "are so popular that when the term \"convolution\"\n",
    "is used without qualifier in a neural network context,\n",
    "it can be taken to mean two-dimensional convolutions.\n",
    "\n",
    "Where `Linear` layers took in batches of vectors of a fixed size\n",
    "and returned batches of vectors of a fixed size,\n",
    "`Conv2d` layers take in batches of two-dimensional _stacked feature maps_\n",
    "and return batches of two-dimensional stacked feature maps.\n",
    "\n",
    "A pseudocode type signature based on\n",
    "[`torchtyping`](https://github.com/patrick-kidger/torchtyping)\n",
    "might look like:\n",
    "\n",
    "```python\n",
    "StackedFeatureMapIn = torch.Tensor[\"batch\", \"in_channels\", \"in_height\", \"in_width\"]\n",
    "StackedFeatureMapOut = torch.Tensor[\"batch\", \"out_channels\", \"out_height\", \"out_width\"]\n",
    "def same_convolution_2d(x: StackedFeatureMapIn) -> StackedFeatureMapOut:\n",
    "```\n",
    "Here, \"map\" is meant to evoke space:\n",
    "our feature maps tell us where\n",
    "features are spatially located.\n",
    "\n",
    "An RGB image is a stacked feature map.\n",
    "It is composed of three feature maps.\n",
    "The first tells us where the \"red\" feature is present,\n",
    "the second \"green\", the third \"blue\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(\n",
    "    url=\"imgs/RGB_channels_separation.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply a convolutional layer to a stacked feature map with some number of channels,\n",
    "we get back a stacked feature map with some number of channels.\n",
    "\n",
    "This output is also a stack of feature maps,\n",
    "and so it is a perfectly acceptable\n",
    "input to another convolutional layer.\n",
    "That means we can compose convolutional layers together,\n",
    "just as we composed generic linear layers together.\n",
    "We again weave non-linear functions in between our linear convolutions,\n",
    "creating a _convolutional neural network_, or CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks build up visual understanding layer by layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the equivalent of the labels, red/green/blue,\n",
    "for the channels in these feature maps?\n",
    "What does a high activation in some position in channel 32\n",
    "of the fifteenth layer of my network tell me?\n",
    "\n",
    "There is no guaranteed way to automatically determine the answer,\n",
    "nor is there a guarantee that the result is human-interpretable.\n",
    "OpenAI's Clarity team spent several years \"reverse engineering\"\n",
    "state-of-the-art convolutiuonal neural networks trained on photographs\n",
    "and found that many of these channels are\n",
    "[directly interpretable](https://distill.pub/2018/building-blocks/).\n",
    "\n",
    "For example, they found that if they pass an image through\n",
    "[GoogLeNet](https://doi.org/10.1109/cvpr.2015.7298594),\n",
    "aka InceptionV1,\n",
    "the winner of the\n",
    "[2014 ImageNet Very Large Scale Visual Recognition Challenge](https://www.image-net.org/challenges/LSVRC/2014/),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample image, taken from https://distill.pub/2018/building-blocks/examples/input_images/dog_cat.jpeg\n",
    "display.Image(\n",
    "    url=\"imgs/dog_cat.jpeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the features become increasingly complex,\n",
    "with channels in early layers (left)\n",
    "acting as maps for simple things like \"high frequency power\" or \"45 degree black-white edge\"\n",
    "and channels in later layers (to right)\n",
    "acting as feature maps for increasingly abstract concepts,\n",
    "like \"circle\" and eventually \"floppy round ear\" or \"pointy ear\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://distill.pub/2018/building-blocks/\n",
    "display.Image(\n",
    "    url=\"imgs/distill-feature-attrib.png\",\n",
    "    width=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <small> The small square images depict a heuristic estimate\n",
    "of what the entire collection of feature maps\n",
    "at a given layer represent (layer IDs at bottom).\n",
    "They are arranged in a spatial grid and their sizes represent\n",
    "the total magnitude of the layer's activations at that position.\n",
    "For details and interactivity, see\n",
    "[the original Distill article](https://distill.pub/2018/building-blocks/).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the\n",
    "[Circuits Thread](https://distill.pub/2020/circuits/)\n",
    "blogpost series,\n",
    "the Open AI Clarity team\n",
    "combines careful examination of weights\n",
    "with direct experimentation\n",
    "to build an understanding of how these higher-level features\n",
    "are constructed in GoogLeNet.\n",
    "\n",
    "For example,\n",
    "they are able to provide reasonable interpretations for\n",
    "[almost every channel in the first five layers](https://distill.pub/2020/circuits/early-vision/).\n",
    "\n",
    "The cell below will pull down their \"weight explorer\"\n",
    "and embed it in this notebook.\n",
    "By default, it starts on\n",
    "[the 52nd channel in the `conv2d1` layer](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d1_52.html),\n",
    "which constructs a large, phase-invariant\n",
    "[Gabor filter](https://en.wikipedia.org/wiki/Gabor_filter)\n",
    "from smaller, phase-sensitive filters.\n",
    "It is in turn used to construct\n",
    "[curve](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_180.html)\n",
    "and\n",
    "[texture](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_114.html)\n",
    "detectors --\n",
    "click on any image to navigate to the weight explorer page\n",
    "for that channel\n",
    "or change the `layer` and `idx`\n",
    "arguments.\n",
    "For additional context,\n",
    "check out the\n",
    "[Early Vision in InceptionV1 blogpost](https://distill.pub/2020/circuits/early-vision/).\n",
    "\n",
    "Click the \"View this neuron in the OpenAI Microscope\" link\n",
    "for an even richer interactive view,\n",
    "including activations on sample images\n",
    "([example](https://microscope.openai.com/models/inceptionv1/conv2d1_0/52)).\n",
    "\n",
    "The\n",
    "[Circuits Thread](https://distill.pub/2020/circuits/)\n",
    "which this explorer accompanies\n",
    "is chock-full of empirical observations, theoretical speculation, and nuggets of wisdom\n",
    "that are invaluable for developing intuition about both\n",
    "convolutional networks in particular and visual perception in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\"conv2d0\", \"conv2d1\", \"conv2d2\", \"mixed3a\", \"mixed3b\"]\n",
    "layer = layers[1]\n",
    "idx = 52\n",
    "\n",
    "weight_explorer = display.IFrame(\n",
    "    src=f\"https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/{layer}_{idx}.html\",\n",
    "    width=1024,\n",
    "    height=720,\n",
    ")\n",
    "weight_explorer.iframe = 'style=\"background: #FFF\";\\n><'.join(\n",
    "    weight_explorer.iframe.split(\"><\")\n",
    ")  # inject background color\n",
    "weight_explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design considerations for CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the release of AlexNet,\n",
    "there has been a feverish decade of engineering and innovation in CNNs --\n",
    "[dilated convolutions](https://arxiv.org/abs/1511.07122),\n",
    "[residual connections](https://arxiv.org/abs/1512.03385), and\n",
    "[batch normalization](https://arxiv.org/abs/1502.03167)\n",
    "came out in 2015 alone, and\n",
    "[work continues](https://arxiv.org/abs/2201.03545) --\n",
    "so we can only scratch the surface in this course and\n",
    "[the devil is in the details](https://arxiv.org/abs/1405.3531v4).\n",
    "\n",
    "The progress of DNNs in general and CNNs in particular\n",
    "has been mostly evolutionary,\n",
    "with lots of good ideas that didn't work out\n",
    "and weird hacks that stuck around because they did.\n",
    "That can make it very hard to design a fresh architecture\n",
    "from first principles that's anywhere near as effective as existing architectures.\n",
    "You're better off tweaking and mutating an existing architecture\n",
    "than trying to design one yourself.\n",
    "\n",
    "If you're not keeping close tabs on the field,\n",
    "when your first start looking for an architecture to base your work off of\n",
    "it's best to go to trusted aggregators, like\n",
    "[Torch IMage Models](https://github.com/rwightman/pytorch-image-models),\n",
    "or `timm`, on GitHub, or\n",
    "[Papers With Code](https://paperswithcode.com),\n",
    "specifically the section for\n",
    "[computer vision](https://paperswithcode.com/methods/area/computer-vision).\n",
    "You can also take a more bottom-up approach by checking\n",
    "the leaderboards of the latest\n",
    "[Kaggle competitions on computer vision](https://www.kaggle.com/competitions?searchQuery=computer+vision).\n",
    "\n",
    "We'll briefly touch here on some of the main design considerations\n",
    "with classic CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapes and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `.forward` pass of the `CNN`,\n",
    "we've included comments that indicate the expected shapes\n",
    "of tensors after each line that changes the shape.\n",
    "\n",
    "Tracking and correctly handling shapes is one of the bugbears\n",
    "of CNNs, especially architectures,\n",
    "like LeNet/AlexNet, that include MLP components\n",
    "that can only operate on fixed-shape tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Shape arithmetic gets pretty hairy pretty fast](https://arxiv.org/abs/1603.07285)\n",
    "if you're supporting the wide variety of convolutions.\n",
    "\n",
    "The easiest way to avoid shape bugs is to keep things simple:\n",
    "choose your convolution parameters,\n",
    "like `padding` and `stride`,\n",
    "to keep the shape the same before and after\n",
    "the convolution.\n",
    "\n",
    "That's what we do, by choosing `padding=1`\n",
    "for `kernel_size=3` and `stride=1`.\n",
    "With unit strides and odd-numbered kernel size,\n",
    "the padding that keeps\n",
    "the input the same size is `kernel_size // 2`.\n",
    "\n",
    "As shapes change, so does the amount of GPU memory taken up by the tensors.\n",
    "Keeping sizes fixed within a block removes one axis of variation\n",
    "in the demands on an important resource.\n",
    "\n",
    "After applying our pooling layer,\n",
    "we can just increase the number of kernels by the right factor\n",
    "to keep total tensor size,\n",
    "and thus memory footprint, constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters, computation, and bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we review the `num`ber of `el`ements in each of the layers,\n",
    "we see that one layer has far more entries than all the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from src.data import mnist\n",
    "from src.models.cnn import CNN\n",
    "\n",
    "cnn = CNN(mnist.MNIST(Namespace()).config())\n",
    "\n",
    "[\n",
    "    p.numel() for p in cnn.parameters()\n",
    "]  # conv weight + bias, conv weight + bias, fc weight + bias, fc weight + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest layer is typically\n",
    "the one in between the convolutional component\n",
    "and the MLP component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_layer = [\n",
    "    p for p in cnn.parameters() if p.numel() == max(p.numel() for p in cnn.parameters())\n",
    "][0]\n",
    "biggest_layer.shape, cnn.fc_input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer dominates the cost of storing the network on disk.\n",
    "That makes it a common target for\n",
    "regularization techniques like DropOut\n",
    "(as in our architecture)\n",
    "and performance optimizations like\n",
    "[pruning](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html).\n",
    "\n",
    "Heuristically, we often associated more parameters with more computation.\n",
    "But just because that layer has the most parameters\n",
    "does not mean that most of the compute time is spent in that layer.\n",
    "\n",
    "Convolutions reuse the same parameters over and over,\n",
    "so the total number of FLOPs done by the layer can be higher\n",
    "than that done by layers with more parameters --\n",
    "much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Linear layers, number of multiplications per input == nparams\n",
    "cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Conv2D layers, it's more complicated\n",
    "\n",
    "\n",
    "def approx_conv_multiplications(\n",
    "    kernel_shape, input_size=(32, 28, 28)\n",
    "):  # this is a rough and dirty approximation\n",
    "    num_kernels, input_channels, kernel_height, kernel_width = kernel_shape\n",
    "    input_height, input_width = input_size[1], input_size[2]\n",
    "\n",
    "    multiplications_per_kernel_application = (\n",
    "        input_channels * kernel_height * kernel_width\n",
    "    )\n",
    "    num_applications = (input_height - kernel_height + 1) * (\n",
    "        input_width - kernel_width + 1\n",
    "    )\n",
    "    mutliplications_per_kernel = (\n",
    "        num_applications * multiplications_per_kernel_application\n",
    "    )\n",
    "\n",
    "    return mutliplications_per_kernel * num_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_conv_multiplications(cnn.conv2.conv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of multiplications in the convolution to multiplications in the fully-connected layer is large!\n",
    "approx_conv_multiplications(cnn.conv2.conv.weight.shape) // cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your compute hardware and the problem characteristics,\n",
    "either the MLP component or the convolutional component\n",
    "could become the critical bottleneck.\n",
    "\n",
    "When you're memory constrained, like when transferring a model \"over the wire\" to a browser,\n",
    "the MLP component is likely to be the bottleneck,\n",
    "whereas when you are compute-constrained, like when running a model on a low-power edge device\n",
    "or in an application with strict low-latency requirements,\n",
    "the convolutional component is likely to be the bottleneck.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs in practice: transfer learning\n",
    "###### This part of the lab is based on PyTorch's transfer Learning for Computer Vision Tutorial by Sasank Chilamkurthy, code available under BSD\n",
    "\n",
    "From [cs231n](https://cs231n.github.io/transfer-learning/):\n",
    "\n",
    "> In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n",
    "\n",
    "The two transfer learning scenarios look as follows:\n",
    "\n",
    "- Finetuning the ConvNet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.\n",
    "- ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()  # interactive mode\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%system curl -O https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "import shutil\n",
    "shutil.unpack_archive('./hymenoptera_data.zip', './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "data_dir = \"data/hymenoptera_data\"\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in [\"train\", \"val\"]\n",
    "}\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=4, shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [\"train\", \"val\"]\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]}\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a few images\n",
    "\n",
    "Let’s visualize a few training images so as to understand the data augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders[\"train\"]))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Now, let’s write a general function to train a model. Here, we will illustrate:\n",
    "\n",
    "- Scheduling the learning rate\n",
    "- Saving the best model\n",
    "\n",
    "In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
    "            print(\"-\" * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in [\"train\", \"val\"]:\n",
    "                if phase == \"train\":\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == \"train\":\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "\n",
    "                print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == \"val\" and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\n",
    "            f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\"\n",
    "        )\n",
    "        print(f\"Best val Acc: {best_acc:4f}\")\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the model predictions\n",
    "\n",
    "Generic function to display predictions for a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders[\"val\"]):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
    "                ax.axis(\"off\")\n",
    "                ax.set_title(f\"predicted: {class_names[preds[j]]}\")\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning the ConvNet\n",
    "\n",
    "With the necessary plumbing out of the way, we can finally start with the actual transfer learning. \n",
    "\n",
    "Our first approach to transfer learning is finetuning: We load a pretrained model and reset the final fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training should take around 15-25 min on CPU. On a GPU it takes less than a minute. On Apple Silicon it should be somewhere in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(\n",
    "    model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, we can visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet as a fixed feature extractor\n",
    "\n",
    "In our second approach to transfer learning, we freeze all layers of the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward().\n",
    "\n",
    "Depending on the task at hand, one could also add more layers after the ConvNet, unfreeze more layers -- the options are endless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a CPU this will take about half the time compared to previous scenario. This is because we only have to compute the gradients for the the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = train_model(\n",
    "    model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
