{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Ray Tune\n",
    "\n",
    "[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) is a library for experiment execution and hyperparameter tuning at any scale. It supports most machine learning frameworks, a variety of state-of-the-art algorithms, and integrates a wide range of dedicated hyperparameter optimization tools.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "There are [six concepts to understand](https://docs.ray.io/en/latest/tune/key-concepts.html):\n",
    "\n",
    "1. Search Spaces: A search space defines the set of valid parameters for your hyperparameters.\n",
    "2. Trainable: In Tune, the objective you want to minimize/maximize is represented by a Trainable. Trainables are [functions](https://docs.ray.io/en/latest/tune/api/trainable.html#tune-function-api) or [classes](https://docs.ray.io/en/latest/tune/api/trainable.html#class-trainable-api) that take hyperparameters as input and return metric.\n",
    "3. Search Algorithms: Search algorithms do the heavy lifting, as they describe _how_ to tune the Trainable.\n",
    "4. Schedulers: Tune can optionally use a Scheduler to stop searches early and thus speed up the hyperparameter search process.\n",
    "5. Trials: Trials are a concrete combination of hyperparameter values.\n",
    "6. Analyses: After the search process has terminated, Tune will present you with a `ResultGrid`, which allows you to access various metrics such as the best available trial or the hyperparameter configuration for said trial.\n",
    "\n",
    "![Tune Flow](imgs/tune_flow.png)\n",
    "\n",
    "Image taken from the [tune documentation](https://docs.ray.io/en/latest/tune/key-concepts.html).\n",
    "\n",
    "With these concepts in mind, we can create a blueprint for any Tune script:\n",
    "\n",
    "```python\n",
    "from ray import tune, train\n",
    "\n",
    "def trainable(config):\n",
    "    # config is a dict containing the hyperparameters, it is a sample from the search space\n",
    "\n",
    "    # train your model using the hyperparameters\n",
    "    # ...\n",
    "    score = 0.5\n",
    "\n",
    "    # return the score\n",
    "    return {\"score\": score}\n",
    "\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    # Your hyperparameters go here\n",
    "}\n",
    "\n",
    "# Select the search algorithm and its parameters\n",
    "# (e.g. Random Search, Bayesian Optimization, HyperBand, etc.; Searcher is the base class for all search algorithms)\n",
    "algo = tune.search.Searcher(\n",
    "    # Your search algorithm parameters go here\n",
    ")\n",
    "\n",
    "# Select the scheduler and its parameters\n",
    "# (e.g. HyperBand, ASHAScheduler, etc.; Scheduler is the base class for all schedulers)\n",
    "scheduler = tune.schedulers.TrialScheduler(\n",
    "    # Your scheduler parameters go here\n",
    ")\n",
    "\n",
    "# Define the tune_config\n",
    "tune_config = tune.TuneConfig(\n",
    "    # Name of the metric that the trainable returns\n",
    "    # and we want to optimize.\n",
    "    metric=\"score\",\n",
    "    # The mode can be \"min\" or \"max\"\n",
    "    mode=\"max\",\n",
    "    # The search algorithm\n",
    "    search_alg=algo,\n",
    "    # The scheduler\n",
    "    scheduler=scheduler,\n",
    "    # Number of times to sample from the hyperparameter space\n",
    "    num_samples=10,\n",
    ")\n",
    "\n",
    "# Define the run_config.\n",
    "run_config = train.RunConfig(stop={\"training_iteration\": 20})\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable=trainable,\n",
    "    tune_config=tune_config,\n",
    "    run_config=run_config,\n",
    "    param_space=search_space,\n",
    ")\n",
    "\n",
    "# Start the search\n",
    "results = tuner.fit() # returns result grid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course very abstract. Let's look at a concrete example: grid search.\n",
    "\n",
    "### Grid Search with Ray Tune\n",
    "\n",
    "In this example, we will be trying to minimize the following objective:\n",
    "\n",
    "```python\n",
    "def trainable(config):\n",
    "    def _loss_fn(step, width, height, activation):\n",
    "        pre_act = (0.1 + width * step / 100) ** (-1) + height * 0.1\n",
    "        if 'relu' == activation:\n",
    "            return F.relu(pre_act)\n",
    "        elif 'tanh' == activation:\n",
    "            return F.sigmoid(pre_act)\n",
    "    # Hyperparameters\n",
    "    width, height, activation = config[\"width\"], config[\"height\"], config[\"activation\"]\n",
    "\n",
    "    for step in range(config[\"steps\"]):\n",
    "        loss = _loss_fn(step, width, height, activation)\n",
    "        train.report({\"iterations\": step, \"mean_loss\": loss})\n",
    "```\n",
    "\n",
    "\n",
    "Grid / random search (implemented by the [`BasicVariantGenerator`](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.search.basic_variant.BasicVariantGenerator.html#ray.tune.search.basic_variant.BasicVariantGenerator)) is the default search algorithm in Ray Tune, it is selected automatically when no search algorithm is passed to the `Tuner`. You can find the complete list of search algorithms [in the docs](https://docs.ray.io/en/latest/tune/api/suggestion.html#random-search-and-grid-search-tune-search-basic-variant-basicvariantgenerator).\n",
    "\n",
    "We will perform a grid search over the `activation`. This means that, for each value, either `relu` or `sigmoid`, we'll randomly sample an equal amount of values for `width`, `height`.\n",
    "\n",
    "Our search space looks as follows:\n",
    "\n",
    "```python\n",
    "    search_space={\n",
    "        \"steps\": 100,  # We don't want to optimize the number of steps.\n",
    "        \"width\": tune.uniform(0, 20),\n",
    "        \"height\": tune.uniform(-100, 100),\n",
    "        \"activation\": tune.grid_search([\"relu\", \"tanh\"]),\n",
    "    }\n",
    "```\n",
    "\n",
    "`tune.uniform` describes a uniform distribution. `tune.grid_search` guarantees that the values are sampled `num_samples` times (`num_samples` is a parameter of [`TunerConfig`](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.TuneConfig.html#ray.tune.TuneConfig), see the blueprint above). For a full list of the random distributions supported by the search space API, refer to the [corresponding page in the documentation](https://docs.ray.io/en/latest/tune/api/search_space.html#tune-search-space-api).\n",
    "\n",
    "The trainable, search algorithm, and search space are everything we need. Let's add them to the blueprint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This code might not work an all Jupyter instances. In that case, try running it on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages directly with pip in current env\n",
    "%pip install -q jupyterlab_widgets\n",
    "%pip install -q ipywidgets\n",
    "%pip install -q data-morph-ai==0.2.0\n",
    "%pip install -q torch==2.6.0\n",
    "%pip install -q ray[tune]==2.41.0\n",
    "%pip install -q hebo==0.3.6\n",
    "%pip install -q prophet==1.1.6\n",
    "%pip install -q mlflow==2.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune, train\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def trainable(config):\n",
    "    def _loss_fn(step, width, height, activation):\n",
    "        pre_act = torch.tensor((0.1 + width * step / 100) ** (-1) + height * 0.1)\n",
    "        if \"relu\" == activation:\n",
    "            return F.relu(pre_act)\n",
    "        elif \"tanh\" == activation:\n",
    "            return F.tanh(pre_act)\n",
    "\n",
    "    # Hyperparameters\n",
    "    width, height, activation = config[\"width\"], config[\"height\"], config[\"activation\"]\n",
    "\n",
    "    for step in range(config[\"steps\"]):\n",
    "        loss = _loss_fn(step, width, height, activation)\n",
    "        train.report({\"iterations\": step, \"mean_loss\": loss.item()})\n",
    "\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    \"steps\": 100,\n",
    "    \"width\": tune.uniform(0, 20),\n",
    "    \"height\": tune.uniform(-100, 100),\n",
    "    \"activation\": tune.grid_search([\"relu\", \"tanh\"]),\n",
    "}\n",
    "\n",
    "# Select the search algorithm and its parameters\n",
    "algo = tune.search.basic_variant.BasicVariantGenerator()\n",
    "\n",
    "# We're not using a scheduler in this example\n",
    "scheduler = None\n",
    "\n",
    "# Define the tune_config\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"mean_loss\",\n",
    "    mode=\"min\",\n",
    "    search_alg=algo,\n",
    "    scheduler=scheduler,\n",
    "    num_samples=20,\n",
    ")\n",
    "\n",
    "# Define the run_config.\n",
    "run_config = train.RunConfig(name=\"Grid search experiment\")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable=trainable,\n",
    "    tune_config=tune_config,\n",
    "    run_config=run_config,\n",
    "    param_space=search_space,\n",
    ")\n",
    "\n",
    "# Start the search\n",
    "results = tuner.fit()  # returns result grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the best run via `ResultGrid.get_best_result()`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get_best_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the corresponding configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get_best_result().config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walking on egg shells\n",
    "\n",
    "Now it's your turn. Use the blueprint above to optimize the following function:\n",
    "\n",
    "$$\n",
    "f(x, y) = -(y + 47) \\cdot \\sin\\left(\\sqrt{\\left| \\frac{x}{2} + (y + 47) \\right|}\\right) - x \\cdot \\sin\\left(\\sqrt{\\left| x - (y + 47) \\right|}\\right)\n",
    "$$\n",
    "\n",
    "also known as the Eggholder function. The search domain is $-512 \\leq x, y \\leq 512$.\n",
    "\n",
    "Implement the function as a trainable and find $(x, y)$ which minimize the eggholder. Use the [`HEBOSearch`](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.search.hebo.HEBOSearch.html#ray.tune.search.hebo.HEBOSearch) search algorithm.\n",
    "\n",
    "_Note: One could of course use a gradient-based optimizer to minimize this function - but that's not the point here. ;)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune, train\n",
    "from ray.tune.search.hebo import HEBOSearch\n",
    "import numpy as np\n",
    "\n",
    "def trainable(config):\n",
    "    x, y = config[\"x\"], config[\"y\"]\n",
    "\n",
    "    def eggholder(x, y):\n",
    "        return -(y + 47) * np.sin(np.sqrt(abs(x / 2 + (y + 47)))) - x * np.sin(np.sqrt(abs(x - (y + 47))))\n",
    "\n",
    "    score = eggholder(x, y)\n",
    "    return {\"score\": score}\n",
    "\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    \"x\": tune.uniform(-512, 512),\n",
    "    \"y\": tune.uniform(-512, 512),\n",
    "}\n",
    "\n",
    "# Select the search algorithm and its parameters\n",
    "# (e.g. Random Search, Bayesian Optimization, HyperBand, etc.; Searcher is the base class for all search algorithms)\n",
    "algo = HEBOSearch()\n",
    "\n",
    "# Select the scheduler and its parameters\n",
    "# (e.g. HyperBand, ASHAScheduler, etc.; Scheduler is the base class for all schedulers)\n",
    "scheduler = None  # We're not using a scheduler in this example.\n",
    "\n",
    "# Define the tune_config\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"score\",\n",
    "    mode=\"min\",\n",
    "    search_alg=algo,\n",
    "    scheduler=scheduler,\n",
    "    num_samples=10,  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Define the run_config.\n",
    "run_config = train.RunConfig(stop={\"training_iteration\": 20})\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable=trainable,\n",
    "    tune_config=tune_config,\n",
    "    run_config=run_config,\n",
    "    param_space=search_space,\n",
    ")\n",
    "\n",
    "# Start the search\n",
    "results = tuner.fit() # returns result grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking hyperparameter experiments\n",
    "\n",
    "Ray Tune integrates with a range of experiment tracking tools, including MLflow. To integrate MLflow (or any tracking framework for that matter), there are two options.\n",
    "\n",
    "1. the Callback API\n",
    "2. the `setup_<integration>` function\n",
    "\n",
    "The callback API is easier to set up but gives you slightly less control over what is logged to MLflow than `setup_mlflow`.\n",
    "We'll only show how to use the callback API, but you are of course invited to explore the `setup_*` option on your own.\n",
    "\n",
    "Using the callback API is as easy as adding the callback to the `RunConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "\n",
    "run_config_with_callback = train.RunConfig(\n",
    "    name=\"MLFlow logging experiment\",\n",
    "    callbacks=[MLflowLoggerCallback(\n",
    "        tracking_uri=\"http://localhost:8080\",  # Replace with your MLFlow tracking server URI.\n",
    "        experiment_name=\"ray-tune-experiments\",\n",
    "        save_artifact=True\n",
    "    )],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else is the same as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable(config):\n",
    "    def _loss_fn(step, width, height, activation):\n",
    "        pre_act = torch.tensor((0.1 + width * step / 100) ** (-1) + height * 0.1)\n",
    "        if \"relu\" == activation:\n",
    "            return F.relu(pre_act)\n",
    "        elif \"tanh\" == activation:\n",
    "            return F.tanh(pre_act)\n",
    "\n",
    "    # Hyperparameters\n",
    "    width, height, activation = config[\"width\"], config[\"height\"], config[\"activation\"]\n",
    "\n",
    "    for step in range(config[\"steps\"]):\n",
    "        loss = _loss_fn(step, width, height, activation)\n",
    "        train.report({\"iterations\": step, \"mean_loss\": loss.item()})\n",
    "\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    \"steps\": 100,\n",
    "    \"width\": tune.uniform(0, 20),\n",
    "    \"height\": tune.uniform(-100, 100),\n",
    "    \"activation\": tune.grid_search([\"relu\", \"tanh\"]),\n",
    "}\n",
    "\n",
    "# Select the search algorithm and its parameters\n",
    "algo = tune.search.basic_variant.BasicVariantGenerator()\n",
    "\n",
    "# We're not using a scheduler in this example\n",
    "scheduler = None\n",
    "\n",
    "# Define the tune_config\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"mean_loss\",\n",
    "    mode=\"min\",\n",
    "    search_alg=algo,\n",
    "    scheduler=scheduler,\n",
    "    num_samples=50,\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable=trainable,\n",
    "    tune_config=tune_config,\n",
    "    run_config=run_config_with_callback,\n",
    "    param_space=search_space,\n",
    ")\n",
    "\n",
    "# Start the search\n",
    "results = tuner.fit()  # returns result grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you head over to your MLFlow tracking server, you should see the experiment and the runs that were logged.\n",
    "\n",
    "![Ray Tune experiment in MLflow](imgs/mlflow_raytune_experiment.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-lab-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
