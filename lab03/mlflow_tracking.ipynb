{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Experiment Management\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "- How experiment management brings observability to ML model development\n",
    "- Workflows for using MLFlow in experiment management, including metric logging, artifact versioning, and hyperparameter optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Management with MLFLow\n",
    "\n",
    "We will be using MLflow Tracking for experiment management. The MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results.\n",
    "\n",
    "There are two important concepts:\n",
    "\n",
    "- **Runs**: Runs are executions of some piece of data science code (e.g. `python train.py`). Each run records metadata (metrics, parameters, start and end times) and as well as the artifacts produced by the code (e.g. model weights).\n",
    "- **Experiments**: An experiment groups together runs for a specific task.\n",
    "\n",
    "![MLFlow concepts](https://mlflow.org/docs/latest/_images/tracking-basics.png)\n",
    "\n",
    "### Launching the MLflow tracking server\n",
    "\n",
    "There are various deployment configurations possible for MLflow. Here we'll simply run it locally, and store everything to local files, but a production setup would usually use cloud storage for artifacts and a database for metadata.\n",
    "\n",
    "![MLflow tracking server setups](https://mlflow.org/docs/latest/_images/tracking-setup-overview.png)\n",
    "\n",
    "To start a local tracking server, run the following in a shell:\n",
    "\n",
    "```shell\n",
    "mlflow server --host 127.0.0.1 --port 8080\n",
    "```\n",
    "\n",
    "### Using the MLflow Client API\n",
    "\n",
    "The `MlflowClient` is one of the primary mechanisms that you will use when training ML models. It enables you to\n",
    "\n",
    "- create new experiments\n",
    "- start runs within experiments\n",
    "- document parameters and metrics for your runs\n",
    "- log artifacts linked to your runs\n",
    "\n",
    "First, import the `MlflowClient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `MlfLowClient` will designate local storage as the tracking server. This means that your experiments, data, models, and everything else you log to MLflow will be stored within the current working directory.\n",
    "\n",
    "To connect to a tracking server, you can set the `tracking_uri` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Default Experiment\n",
    "\n",
    "The Default Experiment is a placeholder that will be used if no explicit experiment is declared. It acts as a fallback for you to ensure that your valuable tracking data is not lost, even if you forget so explicitly create an experiment.\n",
    "\n",
    "Let's see what this default experiment looks like. We can search the available experiments using `MlflowClient.search_experiments()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1706781224823, experiment_id='0', last_update_time=1706781224823, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = client.search_experiments()\n",
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, `search_experiments` returns a list of `Experiment` objects. `Experiment`s come an ID (`experiment_id`), a storage location for their artifacts (`artifact_location`) and a couple of time stamps - and tags. Tags allow you to attach more information to an experiment. The UI allows you to search for these tags. One \"special\" tag is `mlflow.note.content`, which you can use to attach a note to your experiment.\n",
    "\n",
    "#### Creating an experiment\n",
    "\n",
    "Creating an experiment is straightforward. In the following cell, we demonstrate how to create an experiment with additional metadata attached to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"This is an experiment for a coffee shop to forecast sales.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"coffee-forecasting\",\n",
    "    \"team\": \"stores-ml\",\n",
    "    \"project_quarter\": \"Q1-2024\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "produce_apples_experiment = client.create_experiment(\n",
    "    name=\"Coffee_Models\", tags=experiment_tags\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the cell above, head over to your MLflow instance. You should see a new experiment in the `Experiments` menu.\n",
    "\n",
    "![image.png](imgs/important_ui_components.png)\n",
    "\n",
    "There are a couple of UI components that are noteworthy here:\n",
    "\n",
    "![image.png](imgs/important_ui_concepts_annotated.png)\n",
    "\n",
    "As you can see, some of the tags we set previously are visible in the UI. Others are not, but they can still be searched using the search mask or the API. You can search experiments using tasks by setting the `filter_string`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/818393366772456224', creation_time=1706790366452, experiment_id='818393366772456224', last_update_time=1706790366452, lifecycle_stage='active', name='Coffee_Models', tags={'mlflow.note.content': 'This is an experiment for a coffee shop to forecast '\n",
       "                         'sales.',\n",
       "  'project_name': 'coffee-forecasting',\n",
       "  'project_quarter': 'Q1-2024',\n",
       "  'team': 'stores-ml'}>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_experiment = client.search_experiments(filter_string=\"tags.`project_name` = 'coffee-forecasting'\")\n",
    "coffee_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are of course better ways of accessing experiments by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/818393366772456224', creation_time=1706790366452, experiment_id='818393366772456224', last_update_time=1706790366452, lifecycle_stage='active', name='Coffee_Models', tags={'mlflow.note.content': 'This is an experiment for a coffee shop to forecast '\n",
       "                        'sales.',\n",
       " 'project_name': 'coffee-forecasting',\n",
       " 'project_quarter': 'Q1-2024',\n",
       " 'team': 'stores-ml'}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_experiment = client.get_experiment_by_name(\"Coffee_Models\")\n",
    "coffee_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging to Mlflow\n",
    "\n",
    "In this section we'll be taking a closer look at the core features of MLflow Tracking:\n",
    "- creating new runs using the `start_run` context manager\n",
    "- an introduction to logging\n",
    "- the role of model signatures\n",
    "- logging a trained model\n",
    "\n",
    "#### Keeping track of training\n",
    "\n",
    "As an example, we will be forecasting coffee shop sales (a given, after the previous lab) using machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not importing the `MlflowClient` here. Instead, we will be using the `fluent` API. The fluent API is a globally referenced state of the MLFlow tracking server. This global reference is higher-level API to perform the same actions as the `MlflowClient`.\n",
    "\n",
    "To connect to the MLflow tracking server, simply set the tracking URI as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the experiment, run name and artifact path. If you do not set a run name, MLflow will generate one for you.\n",
    "The artifact path is the path that your model will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_experiment = mlflow.set_experiment(\"Coffee_Models\")\n",
    "run_name = \"coffee_forecast_prophet\"\n",
    "artifact_path = \"coffee_prophet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these definitions out of the way, we can now start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:50:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "08:50:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "# We begin with some data wrangling to prepare the data for Prophet\n",
    "df = pd.read_csv(\"data/coffee_sales.csv\")\n",
    "subset = df[(df[\"product_id\"] == 32) & (df[\"store_id\"] == 8)]\n",
    "\n",
    "# For each day in transaction_date, sum the transaction_qty\n",
    "daily_sales = subset.groupby(\"transaction_date\").agg({\"transaction_qty\": \"sum\"}).reset_index()\n",
    "daily_sales.columns = [\"ds\", \"y\"]\n",
    "\n",
    "# Split the last 30 days of data into a test set\n",
    "train = daily_sales.iloc[:-30]\n",
    "test = daily_sales.iloc[-30:].reset_index()\n",
    "\n",
    "\n",
    "# Define hyperparameters for the Prophet model. Their meaning is not important. \n",
    "# We are just demonstrating how to log hyperparameters\n",
    "params = {\n",
    "    \"seasonality_mode\": \"multiplicative\",\n",
    "    \"changepoint_prior_scale\": 0.05,\n",
    "    \"seasonality_prior_scale\": 10.0,\n",
    "    \"holidays_prior_scale\": 10.0,\n",
    "    \"mcmc_samples\": 0,\n",
    "}\n",
    "\n",
    "# Create a Prophet model and fit it to the training data\n",
    "model = Prophet()\n",
    "model.fit(train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "forecast = model.predict(test)\n",
    "\n",
    "# Compare forecasted values to test set\n",
    "mape = (abs(test[\"y\"] - forecast[\"yhat\"]) / test[\"y\"]).mean()\n",
    "rmse = ((test[\"y\"] - forecast[\"yhat\"]) ** 2).mean() ** 0.5\n",
    "metrics = {\"mape\": mape, \"rmse\": rmse}\n",
    "\n",
    "# Start the MLflow run\n",
    "with mlflow.start_run(run_name=run_name, tags={\"model\": \"Prophet\"}) as run:\n",
    "    # Log the model's hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "    # Log the model's metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    # Log the model itself\n",
    "    mlflow.prophet.log_model(model, artifact_path=artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the previous cell:\n",
    "\n",
    "1. We wrangled some data - nothing new here.\n",
    "2. We created a model using the parameters defined in `params` and fit it to the training data.\n",
    "3. We tested it on a test set and computed some metrics.\n",
    "4. This is where it gets interesting from an MLflow perspective: We created a run using the previously defined `run_name` and then logged the `params`, `metrics`, and the `model` itself to MLflow.\n",
    "\n",
    "Note the `mlflow.prophet.log_model` function: MLflow supports a range of machine learning and deep learning frameworks (they call them [\"model flavors\"](https://mlflow.org/docs/latest/models.html#built-in-model-flavors)). If there is an obscure framework they do are not supporting, you can always log [python functions](https://mlflow.org/docs/latest/models.html#python-function-python-function) and raw files directly. Generally, you can log almost everything to MLflow and they offer dedicated functions for a range of artifacts (e.g. matplotlib `Figure`s, images, numpy data). Refer to the [MLflow docs](https://mlflow.org/docs/latest/python_api/mlflow.html) for a complete list.\n",
    "\n",
    "Your `Coffee_Models` experiment should now look something like the screenshot below.\n",
    "\n",
    "![Coffee_Models with content](imgs/Coffee_Models_with_content.png)\n",
    "\n",
    "You can click on the run to reveal detailed information about the run you logged, including the parameters, metrics, and artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with MLflow\n",
    "\n",
    "So far, we've seen a model that is relatively quick to train. Deep learning models, however, can train for days. We'll now see how MLflow can be used to monitor the training of deep models, similar to tools like tensorboard or weights and biases.\n",
    "\n",
    "As an example, let's (try) solve the [(in)famous XOR-problem](https://en.wikipedia.org/wiki/Perceptron#Universal_approximation_theorem) using a pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "torch.manual_seed(2)\n",
    "\n",
    "# Data\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Model\n",
    "class XOR(nn.Module):\n",
    "    def __init__(self, activation=F.sigmoid):\n",
    "        super(XOR, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "# We need room for improvement ;)    \n",
    "activation = F.sigmoid\n",
    "model = XOR(activation=activation)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, nothing changed. You declare your model and loss function, and select an optimizer.\n",
    "\n",
    "They only place that requires some changes is the training loop. Here, we are going to log the training loss with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100000], Loss: 0.2526372969150543\n",
      "Epoch [100/100000], Loss: 0.2500012516975403\n",
      "Epoch [200/100000], Loss: 0.2500007450580597\n",
      "Epoch [300/100000], Loss: 0.2500004470348358\n",
      "Epoch [400/100000], Loss: 0.25000014901161194\n",
      "Epoch [500/100000], Loss: 0.24999982118606567\n",
      "Epoch [600/100000], Loss: 0.24999955296516418\n",
      "Epoch [700/100000], Loss: 0.24999931454658508\n",
      "Epoch [800/100000], Loss: 0.2499990463256836\n",
      "Epoch [900/100000], Loss: 0.2499987930059433\n",
      "Epoch [1000/100000], Loss: 0.2499985545873642\n",
      "Epoch [1100/100000], Loss: 0.2499983012676239\n",
      "Epoch [1200/100000], Loss: 0.2499980926513672\n",
      "Epoch [1300/100000], Loss: 0.24999788403511047\n",
      "Epoch [1400/100000], Loss: 0.24999764561653137\n",
      "Epoch [1500/100000], Loss: 0.24999745190143585\n",
      "Epoch [1600/100000], Loss: 0.24999722838401794\n",
      "Epoch [1700/100000], Loss: 0.24999701976776123\n",
      "Epoch [1800/100000], Loss: 0.24999679625034332\n",
      "Epoch [1900/100000], Loss: 0.24999657273292542\n",
      "Epoch [2000/100000], Loss: 0.2499963641166687\n",
      "Epoch [2100/100000], Loss: 0.24999618530273438\n",
      "Epoch [2200/100000], Loss: 0.24999597668647766\n",
      "Epoch [2300/100000], Loss: 0.24999578297138214\n",
      "Epoch [2400/100000], Loss: 0.24999555945396423\n",
      "Epoch [2500/100000], Loss: 0.24999535083770752\n",
      "Epoch [2600/100000], Loss: 0.2499951273202896\n",
      "Epoch [2700/100000], Loss: 0.2499948889017105\n",
      "Epoch [2800/100000], Loss: 0.249994695186615\n",
      "Epoch [2900/100000], Loss: 0.24999451637268066\n",
      "Epoch [3000/100000], Loss: 0.2499942183494568\n",
      "Epoch [3100/100000], Loss: 0.24999399483203888\n",
      "Epoch [3200/100000], Loss: 0.24999375641345978\n",
      "Epoch [3300/100000], Loss: 0.24999351799488068\n",
      "Epoch [3400/100000], Loss: 0.24999329447746277\n",
      "Epoch [3500/100000], Loss: 0.24999302625656128\n",
      "Epoch [3600/100000], Loss: 0.2499927431344986\n",
      "Epoch [3700/100000], Loss: 0.2499925047159195\n",
      "Epoch [3800/100000], Loss: 0.249992236495018\n",
      "Epoch [3900/100000], Loss: 0.24999192357063293\n",
      "Epoch [4000/100000], Loss: 0.24999164044857025\n",
      "Epoch [4100/100000], Loss: 0.24999135732650757\n",
      "Epoch [4200/100000], Loss: 0.2499910593032837\n",
      "Epoch [4300/100000], Loss: 0.24999070167541504\n",
      "Epoch [4400/100000], Loss: 0.24999040365219116\n",
      "Epoch [4500/100000], Loss: 0.2499900460243225\n",
      "Epoch [4600/100000], Loss: 0.24998967349529266\n",
      "Epoch [4700/100000], Loss: 0.2499893307685852\n",
      "Epoch [4800/100000], Loss: 0.24998894333839417\n",
      "Epoch [4900/100000], Loss: 0.24998852610588074\n",
      "Epoch [5000/100000], Loss: 0.2499881237745285\n",
      "Epoch [5100/100000], Loss: 0.2499876618385315\n",
      "Epoch [5200/100000], Loss: 0.2499871850013733\n",
      "Epoch [5300/100000], Loss: 0.24998673796653748\n",
      "Epoch [5400/100000], Loss: 0.24998627603054047\n",
      "Epoch [5500/100000], Loss: 0.2499857395887375\n",
      "Epoch [5600/100000], Loss: 0.2499852180480957\n",
      "Epoch [5700/100000], Loss: 0.24998463690280914\n",
      "Epoch [5800/100000], Loss: 0.2499840259552002\n",
      "Epoch [5900/100000], Loss: 0.24998340010643005\n",
      "Epoch [6000/100000], Loss: 0.2499827742576599\n",
      "Epoch [6100/100000], Loss: 0.2499820739030838\n",
      "Epoch [6200/100000], Loss: 0.2499813437461853\n",
      "Epoch [6300/100000], Loss: 0.2499806135892868\n",
      "Epoch [6400/100000], Loss: 0.24997982382774353\n",
      "Epoch [6500/100000], Loss: 0.24997898936271667\n",
      "Epoch [6600/100000], Loss: 0.24997811019420624\n",
      "Epoch [6700/100000], Loss: 0.24997715651988983\n",
      "Epoch [6800/100000], Loss: 0.24997618794441223\n",
      "Epoch [6900/100000], Loss: 0.24997512996196747\n",
      "Epoch [7000/100000], Loss: 0.24997404217720032\n",
      "Epoch [7100/100000], Loss: 0.2499728947877884\n",
      "Epoch [7200/100000], Loss: 0.24997161328792572\n",
      "Epoch [7300/100000], Loss: 0.24997036159038544\n",
      "Epoch [7400/100000], Loss: 0.2499689757823944\n",
      "Epoch [7500/100000], Loss: 0.2499675154685974\n",
      "Epoch [7600/100000], Loss: 0.24996595084667206\n",
      "Epoch [7700/100000], Loss: 0.24996426701545715\n",
      "Epoch [7800/100000], Loss: 0.24996253848075867\n",
      "Epoch [7900/100000], Loss: 0.24996063113212585\n",
      "Epoch [8000/100000], Loss: 0.2499586045742035\n",
      "Epoch [8100/100000], Loss: 0.2499564290046692\n",
      "Epoch [8200/100000], Loss: 0.24995413422584534\n",
      "Epoch [8300/100000], Loss: 0.24995164573192596\n",
      "Epoch [8400/100000], Loss: 0.24994900822639465\n",
      "Epoch [8500/100000], Loss: 0.24994614720344543\n",
      "Epoch [8600/100000], Loss: 0.2499430775642395\n",
      "Epoch [8700/100000], Loss: 0.24993979930877686\n",
      "Epoch [8800/100000], Loss: 0.24993620812892914\n",
      "Epoch [8900/100000], Loss: 0.2499324083328247\n",
      "Epoch [9000/100000], Loss: 0.24992823600769043\n",
      "Epoch [9100/100000], Loss: 0.2499236911535263\n",
      "Epoch [9200/100000], Loss: 0.24991881847381592\n",
      "Epoch [9300/100000], Loss: 0.24991348385810852\n",
      "Epoch [9400/100000], Loss: 0.2499076873064041\n",
      "Epoch [9500/100000], Loss: 0.24990135431289673\n",
      "Epoch [9600/100000], Loss: 0.2498944252729416\n",
      "Epoch [9700/100000], Loss: 0.24988684058189392\n",
      "Epoch [9800/100000], Loss: 0.24987852573394775\n",
      "Epoch [9900/100000], Loss: 0.24986937642097473\n",
      "Epoch [10000/100000], Loss: 0.2498592883348465\n",
      "Epoch [10100/100000], Loss: 0.24984808266162872\n",
      "Epoch [10200/100000], Loss: 0.24983572959899902\n",
      "Epoch [10300/100000], Loss: 0.2498220056295395\n",
      "Epoch [10400/100000], Loss: 0.24980668723583221\n",
      "Epoch [10500/100000], Loss: 0.2497895509004593\n",
      "Epoch [10600/100000], Loss: 0.2497703582048416\n",
      "Epoch [10700/100000], Loss: 0.24974875152111053\n",
      "Epoch [10800/100000], Loss: 0.2497244030237198\n",
      "Epoch [10900/100000], Loss: 0.24969682097434998\n",
      "Epoch [11000/100000], Loss: 0.24966539442539215\n",
      "Epoch [11100/100000], Loss: 0.24962955713272095\n",
      "Epoch [11200/100000], Loss: 0.24958845973014832\n",
      "Epoch [11300/100000], Loss: 0.24954111874103546\n",
      "Epoch [11400/100000], Loss: 0.24948635697364807\n",
      "Epoch [11500/100000], Loss: 0.24942269921302795\n",
      "Epoch [11600/100000], Loss: 0.24934832751750946\n",
      "Epoch [11700/100000], Loss: 0.24926093220710754\n",
      "Epoch [11800/100000], Loss: 0.2491576224565506\n",
      "Epoch [11900/100000], Loss: 0.24903492629528046\n",
      "Epoch [12000/100000], Loss: 0.24888819456100464\n",
      "Epoch [12100/100000], Loss: 0.2487117499113083\n",
      "Epoch [12200/100000], Loss: 0.24849826097488403\n",
      "Epoch [12300/100000], Loss: 0.24823831021785736\n",
      "Epoch [12400/100000], Loss: 0.2479199767112732\n",
      "Epoch [12500/100000], Loss: 0.24752792716026306\n",
      "Epoch [12600/100000], Loss: 0.24704265594482422\n",
      "Epoch [12700/100000], Loss: 0.24643953144550323\n",
      "Epoch [12800/100000], Loss: 0.24568746984004974\n",
      "Epoch [12900/100000], Loss: 0.24474839866161346\n",
      "Epoch [13000/100000], Loss: 0.243576318025589\n",
      "Epoch [13100/100000], Loss: 0.24211804568767548\n",
      "Epoch [13200/100000], Loss: 0.24031509459018707\n",
      "Epoch [13300/100000], Loss: 0.23810842633247375\n",
      "Epoch [13400/100000], Loss: 0.2354462593793869\n",
      "Epoch [13500/100000], Loss: 0.23229393362998962\n",
      "Epoch [13600/100000], Loss: 0.22864460945129395\n",
      "Epoch [13700/100000], Loss: 0.22452479600906372\n",
      "Epoch [13800/100000], Loss: 0.21999111771583557\n",
      "Epoch [13900/100000], Loss: 0.2151145040988922\n",
      "Epoch [14000/100000], Loss: 0.2099541276693344\n",
      "Epoch [14100/100000], Loss: 0.2045249044895172\n",
      "Epoch [14200/100000], Loss: 0.19876202940940857\n",
      "Epoch [14300/100000], Loss: 0.19248135387897491\n",
      "Epoch [14400/100000], Loss: 0.18533271551132202\n",
      "Epoch [14500/100000], Loss: 0.1767648607492447\n",
      "Epoch [14600/100000], Loss: 0.16606184840202332\n",
      "Epoch [14700/100000], Loss: 0.1525580883026123\n",
      "Epoch [14800/100000], Loss: 0.13608671724796295\n",
      "Epoch [14900/100000], Loss: 0.11747821420431137\n",
      "Epoch [15000/100000], Loss: 0.09854535013437271\n",
      "Epoch [15100/100000], Loss: 0.08123122155666351\n",
      "Epoch [15200/100000], Loss: 0.06667689979076385\n",
      "Epoch [15300/100000], Loss: 0.05506127327680588\n",
      "Epoch [15400/100000], Loss: 0.046005137264728546\n",
      "Epoch [15500/100000], Loss: 0.038973454385995865\n",
      "Epoch [15600/100000], Loss: 0.033476974815130234\n",
      "Epoch [15700/100000], Loss: 0.029129134491086006\n",
      "Epoch [15800/100000], Loss: 0.02564207836985588\n",
      "Epoch [15900/100000], Loss: 0.022806022316217422\n",
      "Epoch [16000/100000], Loss: 0.02046826109290123\n",
      "Epoch [16100/100000], Loss: 0.018517158925533295\n",
      "Epoch [16200/100000], Loss: 0.016870154067873955\n",
      "Epoch [16300/100000], Loss: 0.01546540018171072\n",
      "Epoch [16400/100000], Loss: 0.01425602100789547\n",
      "Epoch [16500/100000], Loss: 0.013205968774855137\n",
      "Epoch [16600/100000], Loss: 0.012287242338061333\n",
      "Epoch [16700/100000], Loss: 0.011477796360850334\n",
      "Epoch [16800/100000], Loss: 0.010760093107819557\n",
      "Epoch [16900/100000], Loss: 0.010120047256350517\n",
      "Epoch [17000/100000], Loss: 0.009546216577291489\n",
      "Epoch [17100/100000], Loss: 0.009029246866703033\n",
      "Epoch [17200/100000], Loss: 0.008561409078538418\n",
      "Epoch [17300/100000], Loss: 0.00813628826290369\n",
      "Epoch [17400/100000], Loss: 0.007748493459075689\n",
      "Epoch [17500/100000], Loss: 0.007393517531454563\n",
      "Epoch [17600/100000], Loss: 0.007067492697387934\n",
      "Epoch [17700/100000], Loss: 0.006767143961042166\n",
      "Epoch [17800/100000], Loss: 0.006489655468612909\n",
      "Epoch [17900/100000], Loss: 0.00623259786516428\n",
      "Epoch [18000/100000], Loss: 0.005993880797177553\n",
      "Epoch [18100/100000], Loss: 0.00577164813876152\n",
      "Epoch [18200/100000], Loss: 0.005564325489103794\n",
      "Epoch [18300/100000], Loss: 0.005370503291487694\n",
      "Epoch [18400/100000], Loss: 0.005188933108001947\n",
      "Epoch [18500/100000], Loss: 0.00501853134483099\n",
      "Epoch [18600/100000], Loss: 0.004858334083110094\n",
      "Epoch [18700/100000], Loss: 0.004707467742264271\n",
      "Epoch [18800/100000], Loss: 0.004565172828733921\n",
      "Epoch [18900/100000], Loss: 0.004430755041539669\n",
      "Epoch [19000/100000], Loss: 0.004303592722862959\n",
      "Epoch [19100/100000], Loss: 0.00418314291164279\n",
      "Epoch [19200/100000], Loss: 0.0040688905864953995\n",
      "Epoch [19300/100000], Loss: 0.003960387781262398\n",
      "Epoch [19400/100000], Loss: 0.003857219126075506\n",
      "Epoch [19500/100000], Loss: 0.003759015817195177\n",
      "Epoch [19600/100000], Loss: 0.0036654339637607336\n",
      "Epoch [19700/100000], Loss: 0.0035761622712016106\n",
      "Epoch [19800/100000], Loss: 0.0034909199457615614\n",
      "Epoch [19900/100000], Loss: 0.0034094429574906826\n",
      "Epoch [20000/100000], Loss: 0.0033314996398985386\n",
      "Epoch [20100/100000], Loss: 0.0032568643800914288\n",
      "Epoch [20200/100000], Loss: 0.003185343462973833\n",
      "Epoch [20300/100000], Loss: 0.003116743639111519\n",
      "Epoch [20400/100000], Loss: 0.0030508998315781355\n",
      "Epoch [20500/100000], Loss: 0.0029876502230763435\n",
      "Epoch [20600/100000], Loss: 0.00292684743180871\n",
      "Epoch [20700/100000], Loss: 0.0028683501295745373\n",
      "Epoch [20800/100000], Loss: 0.002812046092003584\n",
      "Epoch [20900/100000], Loss: 0.0027578058652579784\n",
      "Epoch [21000/100000], Loss: 0.0027055256068706512\n",
      "Epoch [21100/100000], Loss: 0.0026551014743745327\n",
      "Epoch [21200/100000], Loss: 0.002606443129479885\n",
      "Epoch [21300/100000], Loss: 0.0025594523176550865\n",
      "Epoch [21400/100000], Loss: 0.0025140494108200073\n",
      "Epoch [21500/100000], Loss: 0.002470166888087988\n",
      "Epoch [21600/100000], Loss: 0.002427718136459589\n",
      "Epoch [21700/100000], Loss: 0.0023866482079029083\n",
      "Epoch [21800/100000], Loss: 0.0023468786384910345\n",
      "Epoch [21900/100000], Loss: 0.0023083623964339495\n",
      "Epoch [22000/100000], Loss: 0.002271032892167568\n",
      "Epoch [22100/100000], Loss: 0.0022348430939018726\n",
      "Epoch [22200/100000], Loss: 0.002199739683419466\n",
      "Epoch [22300/100000], Loss: 0.0021656786557286978\n",
      "Epoch [22400/100000], Loss: 0.0021326113492250443\n",
      "Epoch [22500/100000], Loss: 0.002100499579682946\n",
      "Epoch [22600/100000], Loss: 0.002069298177957535\n",
      "Epoch [22700/100000], Loss: 0.0020389750134199858\n",
      "Epoch [22800/100000], Loss: 0.0020094921346753836\n",
      "Epoch [22900/100000], Loss: 0.0019808155484497547\n",
      "Epoch [23000/100000], Loss: 0.0019529152195900679\n",
      "Epoch [23100/100000], Loss: 0.0019257565727457404\n",
      "Epoch [23200/100000], Loss: 0.0018993174890056252\n",
      "Epoch [23300/100000], Loss: 0.0018735649064183235\n",
      "Epoch [23400/100000], Loss: 0.001848475425504148\n",
      "Epoch [23500/100000], Loss: 0.0018240227364003658\n",
      "Epoch [23600/100000], Loss: 0.001800180645659566\n",
      "Epoch [23700/100000], Loss: 0.001776931807398796\n",
      "Epoch [23800/100000], Loss: 0.0017542578279972076\n",
      "Epoch [23900/100000], Loss: 0.0017321283230558038\n",
      "Epoch [24000/100000], Loss: 0.0017105331644415855\n",
      "Epoch [24100/100000], Loss: 0.0016894448781386018\n",
      "Epoch [24200/100000], Loss: 0.0016688595060259104\n",
      "Epoch [24300/100000], Loss: 0.0016487420070916414\n",
      "Epoch [24400/100000], Loss: 0.001629093661904335\n",
      "Epoch [24500/100000], Loss: 0.0016098858322948217\n",
      "Epoch [24600/100000], Loss: 0.001591105479747057\n",
      "Epoch [24700/100000], Loss: 0.0015727533027529716\n",
      "Epoch [24800/100000], Loss: 0.0015547992661595345\n",
      "Epoch [24900/100000], Loss: 0.0015372340567409992\n",
      "Epoch [25000/100000], Loss: 0.0015200478956103325\n",
      "Epoch [25100/100000], Loss: 0.0015032276278361678\n",
      "Epoch [25200/100000], Loss: 0.0014867703430354595\n",
      "Epoch [25300/100000], Loss: 0.0014706478687003255\n",
      "Epoch [25400/100000], Loss: 0.0014548614853993058\n",
      "Epoch [25500/100000], Loss: 0.0014394003665074706\n",
      "Epoch [25600/100000], Loss: 0.0014242490287870169\n",
      "Epoch [25700/100000], Loss: 0.0014094063080847263\n",
      "Epoch [25800/100000], Loss: 0.0013948556734248996\n",
      "Epoch [25900/100000], Loss: 0.00138059351593256\n",
      "Epoch [26000/100000], Loss: 0.0013666140148416162\n",
      "Epoch [26100/100000], Loss: 0.001352903083898127\n",
      "Epoch [26200/100000], Loss: 0.0013394549023360014\n",
      "Epoch [26300/100000], Loss: 0.0013262620195746422\n",
      "Epoch [26400/100000], Loss: 0.0013133181491866708\n",
      "Epoch [26500/100000], Loss: 0.001300611300393939\n",
      "Epoch [26600/100000], Loss: 0.001288144732825458\n",
      "Epoch [26700/100000], Loss: 0.0012759068049490452\n",
      "Epoch [26800/100000], Loss: 0.001263888436369598\n",
      "Epoch [26900/100000], Loss: 0.0012520896270871162\n",
      "Epoch [27000/100000], Loss: 0.0012405060697346926\n",
      "Epoch [27100/100000], Loss: 0.0012291226303204894\n",
      "Epoch [27200/100000], Loss: 0.0012179407058283687\n",
      "Epoch [27300/100000], Loss: 0.0012069535441696644\n",
      "Epoch [27400/100000], Loss: 0.001196156837977469\n",
      "Epoch [27500/100000], Loss: 0.0011855449993163347\n",
      "Epoch [27600/100000], Loss: 0.0011751139536499977\n",
      "Epoch [27700/100000], Loss: 0.001164857647381723\n",
      "Epoch [27800/100000], Loss: 0.001154775032773614\n",
      "Epoch [27900/100000], Loss: 0.0011448597069829702\n",
      "Epoch [28000/100000], Loss: 0.0011351100401952863\n",
      "Epoch [28100/100000], Loss: 0.0011255156714469194\n",
      "Epoch [28200/100000], Loss: 0.001116081839427352\n",
      "Epoch [28300/100000], Loss: 0.0011067957384511828\n",
      "Epoch [28400/100000], Loss: 0.0010976578341796994\n",
      "Epoch [28500/100000], Loss: 0.0010886717354878783\n",
      "Epoch [28600/100000], Loss: 0.001079816953279078\n",
      "Epoch [28700/100000], Loss: 0.0010711044305935502\n",
      "Epoch [28800/100000], Loss: 0.0010625330032780766\n",
      "Epoch [28900/100000], Loss: 0.0010540836956351995\n",
      "Epoch [29000/100000], Loss: 0.001045771175995469\n",
      "Epoch [29100/100000], Loss: 0.0010375834535807371\n",
      "Epoch [29200/100000], Loss: 0.0010295151732861996\n",
      "Epoch [29300/100000], Loss: 0.0010215808870270848\n",
      "Epoch [29400/100000], Loss: 0.0010137436911463737\n",
      "Epoch [29500/100000], Loss: 0.0010060400236397982\n",
      "Epoch [29600/100000], Loss: 0.0009984310017898679\n",
      "Epoch [29700/100000], Loss: 0.0009909548098221421\n",
      "Epoch [29800/100000], Loss: 0.0009835624368861318\n",
      "Epoch [29900/100000], Loss: 0.0009763015550561249\n",
      "Epoch [30000/100000], Loss: 0.0009691223385743797\n",
      "Epoch [30100/100000], Loss: 0.0009620640194043517\n",
      "Epoch [30200/100000], Loss: 0.0009550806134939194\n",
      "Epoch [30300/100000], Loss: 0.0009482222376391292\n",
      "Epoch [30400/100000], Loss: 0.0009414328960701823\n",
      "Epoch [30500/100000], Loss: 0.0009347607265226543\n",
      "Epoch [30600/100000], Loss: 0.000928153982385993\n",
      "Epoch [30700/100000], Loss: 0.0009216684848070145\n",
      "Epoch [30800/100000], Loss: 0.000915241427719593\n",
      "Epoch [30900/100000], Loss: 0.0009089238592423499\n",
      "Epoch [31000/100000], Loss: 0.0009026714833453298\n",
      "Epoch [31100/100000], Loss: 0.0008965161396190524\n",
      "Epoch [31200/100000], Loss: 0.0008904344867914915\n",
      "Epoch [31300/100000], Loss: 0.0008844303665682673\n",
      "Epoch [31400/100000], Loss: 0.0008785149548202753\n",
      "Epoch [31500/100000], Loss: 0.0008726575179025531\n",
      "Epoch [31600/100000], Loss: 0.0008668985683470964\n",
      "Epoch [31700/100000], Loss: 0.0008611927041783929\n",
      "Epoch [31800/100000], Loss: 0.0008555769454687834\n",
      "Epoch [31900/100000], Loss: 0.0008500221883878112\n",
      "Epoch [32000/100000], Loss: 0.0008445237763226032\n",
      "Epoch [32100/100000], Loss: 0.0008391254814341664\n",
      "Epoch [32200/100000], Loss: 0.0008337676990777254\n",
      "Epoch [32300/100000], Loss: 0.0008284868090413511\n",
      "Epoch [32400/100000], Loss: 0.0008232761756516993\n",
      "Epoch [32500/100000], Loss: 0.000818107568193227\n",
      "Epoch [32600/100000], Loss: 0.0008130260393954813\n",
      "Epoch [32700/100000], Loss: 0.0008079919498413801\n",
      "Epoch [32800/100000], Loss: 0.0008030094904825091\n",
      "Epoch [32900/100000], Loss: 0.0007981177768670022\n",
      "Epoch [33000/100000], Loss: 0.0007932551670819521\n",
      "Epoch [33100/100000], Loss: 0.0007884492515586317\n",
      "Epoch [33200/100000], Loss: 0.0007837259909138083\n",
      "Epoch [33300/100000], Loss: 0.000779034337028861\n",
      "Epoch [33400/100000], Loss: 0.0007743966416455805\n",
      "Epoch [33500/100000], Loss: 0.0007698306581005454\n",
      "Epoch [33600/100000], Loss: 0.0007653002976439893\n",
      "Epoch [33700/100000], Loss: 0.0007608194136992097\n",
      "Epoch [33800/100000], Loss: 0.0007564093102701008\n",
      "Epoch [33900/100000], Loss: 0.0007520290673710406\n",
      "Epoch [34000/100000], Loss: 0.0007476936443708837\n",
      "Epoch [34100/100000], Loss: 0.0007434333092533052\n",
      "Epoch [34200/100000], Loss: 0.0007392038824036717\n",
      "Epoch [34300/100000], Loss: 0.0007350019295699894\n",
      "Epoch [34400/100000], Loss: 0.000730878789909184\n",
      "Epoch [34500/100000], Loss: 0.0007267951150424778\n",
      "Epoch [34600/100000], Loss: 0.0007227386813610792\n",
      "Epoch [34700/100000], Loss: 0.0007187346345745027\n",
      "Epoch [34800/100000], Loss: 0.0007147821597754955\n",
      "Epoch [34900/100000], Loss: 0.000710858148522675\n",
      "Epoch [35000/100000], Loss: 0.0007069709245115519\n",
      "Epoch [35100/100000], Loss: 0.0007031479617580771\n",
      "Epoch [35200/100000], Loss: 0.0006993560818955302\n",
      "Epoch [35300/100000], Loss: 0.0006955908611416817\n",
      "Epoch [35400/100000], Loss: 0.000691872788593173\n",
      "Epoch [35500/100000], Loss: 0.0006882031448185444\n",
      "Epoch [35600/100000], Loss: 0.0006845653988420963\n",
      "Epoch [35700/100000], Loss: 0.0006809477927163243\n",
      "Epoch [35800/100000], Loss: 0.0006773913628421724\n",
      "Epoch [35900/100000], Loss: 0.0006738649681210518\n",
      "Epoch [36000/100000], Loss: 0.0006703694816678762\n",
      "Epoch [36100/100000], Loss: 0.0006668963469564915\n",
      "Epoch [36200/100000], Loss: 0.0006634841556660831\n",
      "Epoch [36300/100000], Loss: 0.000660097983200103\n",
      "Epoch [36400/100000], Loss: 0.0006567356176674366\n",
      "Epoch [36500/100000], Loss: 0.0006534002022817731\n",
      "Epoch [36600/100000], Loss: 0.0006501206080429256\n",
      "Epoch [36700/100000], Loss: 0.0006468697101809084\n",
      "Epoch [36800/100000], Loss: 0.0006436370895244181\n",
      "Epoch [36900/100000], Loss: 0.0006404314190149307\n",
      "Epoch [37000/100000], Loss: 0.0006372714415192604\n",
      "Epoch [37100/100000], Loss: 0.0006341448752209544\n",
      "Epoch [37200/100000], Loss: 0.0006310386816039681\n",
      "Epoch [37300/100000], Loss: 0.0006279532099142671\n",
      "Epoch [37400/100000], Loss: 0.0006249005673453212\n",
      "Epoch [37500/100000], Loss: 0.0006219021743163466\n",
      "Epoch [37600/100000], Loss: 0.0006189175182953477\n",
      "Epoch [37700/100000], Loss: 0.0006159493932500482\n",
      "Epoch [37800/100000], Loss: 0.0006129966350272298\n",
      "Epoch [37900/100000], Loss: 0.0006100983591750264\n",
      "Epoch [38000/100000], Loss: 0.0006072309333831072\n",
      "Epoch [38100/100000], Loss: 0.0006043798639439046\n",
      "Epoch [38200/100000], Loss: 0.0006015453254804015\n",
      "Epoch [38300/100000], Loss: 0.000598725164309144\n",
      "Epoch [38400/100000], Loss: 0.0005959642003290355\n",
      "Epoch [38500/100000], Loss: 0.0005932222702540457\n",
      "Epoch [38600/100000], Loss: 0.0005904963472858071\n",
      "Epoch [38700/100000], Loss: 0.0005877856747247279\n",
      "Epoch [38800/100000], Loss: 0.0005850897287018597\n",
      "Epoch [38900/100000], Loss: 0.0005824502441100776\n",
      "Epoch [39000/100000], Loss: 0.0005798317724838853\n",
      "Epoch [39100/100000], Loss: 0.0005772250005975366\n",
      "Epoch [39200/100000], Loss: 0.0005746316746808589\n",
      "Epoch [39300/100000], Loss: 0.0005720528424717486\n",
      "Epoch [39400/100000], Loss: 0.0005695156287401915\n",
      "Epoch [39500/100000], Loss: 0.000567010953091085\n",
      "Epoch [39600/100000], Loss: 0.0005645203636959195\n",
      "Epoch [39700/100000], Loss: 0.0005620432202704251\n",
      "Epoch [39800/100000], Loss: 0.000559579988475889\n",
      "Epoch [39900/100000], Loss: 0.0005571366054937243\n",
      "Epoch [40000/100000], Loss: 0.0005547343753278255\n",
      "Epoch [40100/100000], Loss: 0.0005523469299077988\n",
      "Epoch [40200/100000], Loss: 0.000549978343769908\n",
      "Epoch [40300/100000], Loss: 0.0005476209335029125\n",
      "Epoch [40400/100000], Loss: 0.0005452780169434845\n",
      "Epoch [40500/100000], Loss: 0.0005429660668596625\n",
      "Epoch [40600/100000], Loss: 0.0005406812415458262\n",
      "Epoch [40700/100000], Loss: 0.0005384094547480345\n",
      "Epoch [40800/100000], Loss: 0.0005361491930671036\n",
      "Epoch [40900/100000], Loss: 0.0005339037743397057\n",
      "Epoch [41000/100000], Loss: 0.0005316720344126225\n",
      "Epoch [41100/100000], Loss: 0.0005294763250276446\n",
      "Epoch [41200/100000], Loss: 0.0005273036076687276\n",
      "Epoch [41300/100000], Loss: 0.0005251390975899994\n",
      "Epoch [41400/100000], Loss: 0.0005229886737652123\n",
      "Epoch [41500/100000], Loss: 0.0005208510556258261\n",
      "Epoch [41600/100000], Loss: 0.0005187250208109617\n",
      "Epoch [41700/100000], Loss: 0.000516621395945549\n",
      "Epoch [41800/100000], Loss: 0.0005145468167029321\n",
      "Epoch [41900/100000], Loss: 0.0005124892923049629\n",
      "Epoch [42000/100000], Loss: 0.0005104402080178261\n",
      "Epoch [42100/100000], Loss: 0.0005084031727164984\n",
      "Epoch [42200/100000], Loss: 0.0005063799908384681\n",
      "Epoch [42300/100000], Loss: 0.0005043712444603443\n",
      "Epoch [42400/100000], Loss: 0.0005023872945457697\n",
      "Epoch [42500/100000], Loss: 0.0005004259292036295\n",
      "Epoch [42600/100000], Loss: 0.000498472130857408\n",
      "Epoch [42700/100000], Loss: 0.0004965253174304962\n",
      "Epoch [42800/100000], Loss: 0.0004945939290337265\n",
      "Epoch [42900/100000], Loss: 0.0004926745314151049\n",
      "Epoch [43000/100000], Loss: 0.0004907731199637055\n",
      "Epoch [43100/100000], Loss: 0.000488894060254097\n",
      "Epoch [43200/100000], Loss: 0.0004870357515756041\n",
      "Epoch [43300/100000], Loss: 0.00048518431140109897\n",
      "Epoch [43400/100000], Loss: 0.0004833415732719004\n",
      "Epoch [43500/100000], Loss: 0.00048150605289265513\n",
      "Epoch [43600/100000], Loss: 0.0004796837456524372\n",
      "Epoch [43700/100000], Loss: 0.0004778714501298964\n",
      "Epoch [43800/100000], Loss: 0.000476085115224123\n",
      "Epoch [43900/100000], Loss: 0.0004743206372950226\n",
      "Epoch [44000/100000], Loss: 0.0004725670733023435\n",
      "Epoch [44100/100000], Loss: 0.00047082171658985317\n",
      "Epoch [44200/100000], Loss: 0.00046908194781281054\n",
      "Epoch [44300/100000], Loss: 0.0004673534713219851\n",
      "Epoch [44400/100000], Loss: 0.0004656313394661993\n",
      "Epoch [44500/100000], Loss: 0.0004639264661818743\n",
      "Epoch [44600/100000], Loss: 0.00046223681420087814\n",
      "Epoch [44700/100000], Loss: 0.0004605700960382819\n",
      "Epoch [44800/100000], Loss: 0.0004589114978443831\n",
      "Epoch [44900/100000], Loss: 0.00045726311509497464\n",
      "Epoch [45000/100000], Loss: 0.00045561918523162603\n",
      "Epoch [45100/100000], Loss: 0.00045398742076940835\n",
      "Epoch [45200/100000], Loss: 0.00045236264122650027\n",
      "Epoch [45300/100000], Loss: 0.00045075235539115965\n",
      "Epoch [45400/100000], Loss: 0.0004491570289246738\n",
      "Epoch [45500/100000], Loss: 0.0004475809109862894\n",
      "Epoch [45600/100000], Loss: 0.00044601192348636687\n",
      "Epoch [45700/100000], Loss: 0.00044445283128879964\n",
      "Epoch [45800/100000], Loss: 0.0004428992688190192\n",
      "Epoch [45900/100000], Loss: 0.0004413542046677321\n",
      "Epoch [46000/100000], Loss: 0.0004398177843540907\n",
      "Epoch [46100/100000], Loss: 0.00043828689376823604\n",
      "Epoch [46200/100000], Loss: 0.0004367768997326493\n",
      "Epoch [46300/100000], Loss: 0.00043527415255084634\n",
      "Epoch [46400/100000], Loss: 0.00043379684211686254\n",
      "Epoch [46500/100000], Loss: 0.0004323212488088757\n",
      "Epoch [46600/100000], Loss: 0.0004308536590542644\n",
      "Epoch [46700/100000], Loss: 0.00042939349077641964\n",
      "Epoch [46800/100000], Loss: 0.000427941617090255\n",
      "Epoch [46900/100000], Loss: 0.0004264939052518457\n",
      "Epoch [47000/100000], Loss: 0.0004250535275787115\n",
      "Epoch [47100/100000], Loss: 0.000423631863668561\n",
      "Epoch [47200/100000], Loss: 0.00042221543844789267\n",
      "Epoch [47300/100000], Loss: 0.00042081897845491767\n",
      "Epoch [47400/100000], Loss: 0.0004194291541352868\n",
      "Epoch [47500/100000], Loss: 0.0004180464311502874\n",
      "Epoch [47600/100000], Loss: 0.0004166690050624311\n",
      "Epoch [47700/100000], Loss: 0.0004152992332819849\n",
      "Epoch [47800/100000], Loss: 0.0004139383090659976\n",
      "Epoch [47900/100000], Loss: 0.0004125815466977656\n",
      "Epoch [48000/100000], Loss: 0.0004112396854907274\n",
      "Epoch [48100/100000], Loss: 0.0004099076031707227\n",
      "Epoch [48200/100000], Loss: 0.0004085790424142033\n",
      "Epoch [48300/100000], Loss: 0.0004072696319781244\n",
      "Epoch [48400/100000], Loss: 0.0004059635102748871\n",
      "Epoch [48500/100000], Loss: 0.00040466536302119493\n",
      "Epoch [48600/100000], Loss: 0.0004033762088511139\n",
      "Epoch [48700/100000], Loss: 0.0004020913038402796\n",
      "Epoch [48800/100000], Loss: 0.00040081224869936705\n",
      "Epoch [48900/100000], Loss: 0.0003995371807832271\n",
      "Epoch [49000/100000], Loss: 0.00039827488944865763\n",
      "Epoch [49100/100000], Loss: 0.00039702525828033686\n",
      "Epoch [49200/100000], Loss: 0.000395777024095878\n",
      "Epoch [49300/100000], Loss: 0.0003945433418266475\n",
      "Epoch [49400/100000], Loss: 0.00039332019514404237\n",
      "Epoch [49500/100000], Loss: 0.00039210086106322706\n",
      "Epoch [49600/100000], Loss: 0.0003908872022293508\n",
      "Epoch [49700/100000], Loss: 0.00038968160515651107\n",
      "Epoch [49800/100000], Loss: 0.0003884805482812226\n",
      "Epoch [49900/100000], Loss: 0.00038728222716599703\n",
      "Epoch [50000/100000], Loss: 0.0003860884753521532\n",
      "Epoch [50100/100000], Loss: 0.0003849105560220778\n",
      "Epoch [50200/100000], Loss: 0.0003837418626062572\n",
      "Epoch [50300/100000], Loss: 0.00038257826236076653\n",
      "Epoch [50400/100000], Loss: 0.0003814212104771286\n",
      "Epoch [50500/100000], Loss: 0.00038027777918614447\n",
      "Epoch [50600/100000], Loss: 0.0003791364433709532\n",
      "Epoch [50700/100000], Loss: 0.00037799988058395684\n",
      "Epoch [50800/100000], Loss: 0.0003768716414924711\n",
      "Epoch [50900/100000], Loss: 0.00037574375164695084\n",
      "Epoch [51000/100000], Loss: 0.0003746267466340214\n",
      "Epoch [51100/100000], Loss: 0.00037351035280153155\n",
      "Epoch [51200/100000], Loss: 0.0003723992267623544\n",
      "Epoch [51300/100000], Loss: 0.00037130541750229895\n",
      "Epoch [51400/100000], Loss: 0.00037021361640654504\n",
      "Epoch [51500/100000], Loss: 0.00036912408540956676\n",
      "Epoch [51600/100000], Loss: 0.00036804424598813057\n",
      "Epoch [51700/100000], Loss: 0.00036697875475510955\n",
      "Epoch [51800/100000], Loss: 0.00036591646494343877\n",
      "Epoch [51900/100000], Loss: 0.00036485871532931924\n",
      "Epoch [52000/100000], Loss: 0.0003638064372353256\n",
      "Epoch [52100/100000], Loss: 0.0003627572441473603\n",
      "Epoch [52200/100000], Loss: 0.00036171157262288034\n",
      "Epoch [52300/100000], Loss: 0.00036066846223548055\n",
      "Epoch [52400/100000], Loss: 0.0003596300375647843\n",
      "Epoch [52500/100000], Loss: 0.00035860604839399457\n",
      "Epoch [52600/100000], Loss: 0.0003575879381969571\n",
      "Epoch [52700/100000], Loss: 0.00035657203989103436\n",
      "Epoch [52800/100000], Loss: 0.0003555601870175451\n",
      "Epoch [52900/100000], Loss: 0.00035455715260468423\n",
      "Epoch [53000/100000], Loss: 0.0003535641299095005\n",
      "Epoch [53100/100000], Loss: 0.00035257762647233903\n",
      "Epoch [53200/100000], Loss: 0.00035159586695954204\n",
      "Epoch [53300/100000], Loss: 0.00035061611561104655\n",
      "Epoch [53400/100000], Loss: 0.0003496407298371196\n",
      "Epoch [53500/100000], Loss: 0.0003486677014734596\n",
      "Epoch [53600/100000], Loss: 0.0003476989804767072\n",
      "Epoch [53700/100000], Loss: 0.00034673369373194873\n",
      "Epoch [53800/100000], Loss: 0.00034577780752442777\n",
      "Epoch [53900/100000], Loss: 0.0003448307397775352\n",
      "Epoch [54000/100000], Loss: 0.00034388768835924566\n",
      "Epoch [54100/100000], Loss: 0.00034294716897420585\n",
      "Epoch [54200/100000], Loss: 0.0003420087741687894\n",
      "Epoch [54300/100000], Loss: 0.00034108167164959013\n",
      "Epoch [54400/100000], Loss: 0.000340159545885399\n",
      "Epoch [54500/100000], Loss: 0.0003392435028217733\n",
      "Epoch [54600/100000], Loss: 0.0003383301373105496\n",
      "Epoch [54700/100000], Loss: 0.00033742052619345486\n",
      "Epoch [54800/100000], Loss: 0.0003365158918313682\n",
      "Epoch [54900/100000], Loss: 0.0003356124507263303\n",
      "Epoch [55000/100000], Loss: 0.0003347145102452487\n",
      "Epoch [55100/100000], Loss: 0.0003338181704748422\n",
      "Epoch [55200/100000], Loss: 0.00033293067826889455\n",
      "Epoch [55300/100000], Loss: 0.0003320506657473743\n",
      "Epoch [55400/100000], Loss: 0.00033117260318249464\n",
      "Epoch [55500/100000], Loss: 0.0003303012053947896\n",
      "Epoch [55600/100000], Loss: 0.0003294315538369119\n",
      "Epoch [55700/100000], Loss: 0.0003285661805421114\n",
      "Epoch [55800/100000], Loss: 0.000327711139107123\n",
      "Epoch [55900/100000], Loss: 0.0003268611035309732\n",
      "Epoch [56000/100000], Loss: 0.00032601694692857563\n",
      "Epoch [56100/100000], Loss: 0.0003251740417908877\n",
      "Epoch [56200/100000], Loss: 0.0003243343671783805\n",
      "Epoch [56300/100000], Loss: 0.00032349524553865194\n",
      "Epoch [56400/100000], Loss: 0.0003226591506972909\n",
      "Epoch [56500/100000], Loss: 0.00032182742143049836\n",
      "Epoch [56600/100000], Loss: 0.0003209980786778033\n",
      "Epoch [56700/100000], Loss: 0.0003201736544724554\n",
      "Epoch [56800/100000], Loss: 0.00031936090090312064\n",
      "Epoch [56900/100000], Loss: 0.00031854878761805594\n",
      "Epoch [57000/100000], Loss: 0.0003177383332513273\n",
      "Epoch [57100/100000], Loss: 0.00031693081837147474\n",
      "Epoch [57200/100000], Loss: 0.0003161261847708374\n",
      "Epoch [57300/100000], Loss: 0.00031532353023067117\n",
      "Epoch [57400/100000], Loss: 0.0003145346709061414\n",
      "Epoch [57500/100000], Loss: 0.00031375198159366846\n",
      "Epoch [57600/100000], Loss: 0.00031297269742935896\n",
      "Epoch [57700/100000], Loss: 0.00031219550874084234\n",
      "Epoch [57800/100000], Loss: 0.000311422161757946\n",
      "Epoch [57900/100000], Loss: 0.00031065125949680805\n",
      "Epoch [58000/100000], Loss: 0.0003098819579463452\n",
      "Epoch [58100/100000], Loss: 0.0003091138496529311\n",
      "Epoch [58200/100000], Loss: 0.0003083475457970053\n",
      "Epoch [58300/100000], Loss: 0.0003075846179854125\n",
      "Epoch [58400/100000], Loss: 0.0003068311489187181\n",
      "Epoch [58500/100000], Loss: 0.0003060849558096379\n",
      "Epoch [58600/100000], Loss: 0.00030533879180438817\n",
      "Epoch [58700/100000], Loss: 0.0003045961493626237\n",
      "Epoch [58800/100000], Loss: 0.00030385464197024703\n",
      "Epoch [58900/100000], Loss: 0.0003031136002391577\n",
      "Epoch [59000/100000], Loss: 0.00030237672035582364\n",
      "Epoch [59100/100000], Loss: 0.00030164283816702664\n",
      "Epoch [59200/100000], Loss: 0.0003009217616636306\n",
      "Epoch [59300/100000], Loss: 0.000300201732898131\n",
      "Epoch [59400/100000], Loss: 0.00029948746669106185\n",
      "Epoch [59500/100000], Loss: 0.00029877241468057036\n",
      "Epoch [59600/100000], Loss: 0.00029806175734847784\n",
      "Epoch [59700/100000], Loss: 0.00029735008138231933\n",
      "Epoch [59800/100000], Loss: 0.0002966421307064593\n",
      "Epoch [59900/100000], Loss: 0.0002959356061182916\n",
      "Epoch [60000/100000], Loss: 0.0002952314680442214\n",
      "Epoch [60100/100000], Loss: 0.00029452945454977453\n",
      "Epoch [60200/100000], Loss: 0.0002938409452326596\n",
      "Epoch [60300/100000], Loss: 0.00029315458959899843\n",
      "Epoch [60400/100000], Loss: 0.00029246966005302966\n",
      "Epoch [60500/100000], Loss: 0.0002917866222560406\n",
      "Epoch [60600/100000], Loss: 0.0002911054471042007\n",
      "Epoch [60700/100000], Loss: 0.00029042636742815375\n",
      "Epoch [60800/100000], Loss: 0.0002897508966270834\n",
      "Epoch [60900/100000], Loss: 0.0002890769683290273\n",
      "Epoch [61000/100000], Loss: 0.00028840883169323206\n",
      "Epoch [61100/100000], Loss: 0.0002877500664908439\n",
      "Epoch [61200/100000], Loss: 0.00028709301841445267\n",
      "Epoch [61300/100000], Loss: 0.00028643760015256703\n",
      "Epoch [61400/100000], Loss: 0.0002857828512787819\n",
      "Epoch [61500/100000], Loss: 0.0002851306344382465\n",
      "Epoch [61600/100000], Loss: 0.00028448039665818214\n",
      "Epoch [61700/100000], Loss: 0.0002838321088347584\n",
      "Epoch [61800/100000], Loss: 0.00028318719705566764\n",
      "Epoch [61900/100000], Loss: 0.0002825412666425109\n",
      "Epoch [62000/100000], Loss: 0.00028189847944304347\n",
      "Epoch [62100/100000], Loss: 0.00028126122197136283\n",
      "Epoch [62200/100000], Loss: 0.000280632491922006\n",
      "Epoch [62300/100000], Loss: 0.00028000326710753143\n",
      "Epoch [62400/100000], Loss: 0.00027937666163779795\n",
      "Epoch [62500/100000], Loss: 0.00027875028899870813\n",
      "Epoch [62600/100000], Loss: 0.0002781250805128366\n",
      "Epoch [62700/100000], Loss: 0.00027750327717512846\n",
      "Epoch [62800/100000], Loss: 0.00027688121190294623\n",
      "Epoch [62900/100000], Loss: 0.00027626092196442187\n",
      "Epoch [63000/100000], Loss: 0.00027564572519622743\n",
      "Epoch [63100/100000], Loss: 0.00027504010358825326\n",
      "Epoch [63200/100000], Loss: 0.00027444117586128414\n",
      "Epoch [63300/100000], Loss: 0.0002738433831837028\n",
      "Epoch [63400/100000], Loss: 0.0002732474822551012\n",
      "Epoch [63500/100000], Loss: 0.00027265140670351684\n",
      "Epoch [63600/100000], Loss: 0.0002720584161579609\n",
      "Epoch [63700/100000], Loss: 0.0002714665897656232\n",
      "Epoch [63800/100000], Loss: 0.0002708756073843688\n",
      "Epoch [63900/100000], Loss: 0.0002702847705222666\n",
      "Epoch [64000/100000], Loss: 0.00026969556347467005\n",
      "Epoch [64100/100000], Loss: 0.00026910906308330595\n",
      "Epoch [64200/100000], Loss: 0.00026852564769797027\n",
      "Epoch [64300/100000], Loss: 0.0002679532626643777\n",
      "Epoch [64400/100000], Loss: 0.00026738198357634246\n",
      "Epoch [64500/100000], Loss: 0.00026681210147216916\n",
      "Epoch [64600/100000], Loss: 0.00026624303427524865\n",
      "Epoch [64700/100000], Loss: 0.0002656753931660205\n",
      "Epoch [64800/100000], Loss: 0.00026510993484407663\n",
      "Epoch [64900/100000], Loss: 0.0002645439235493541\n",
      "Epoch [65000/100000], Loss: 0.0002639798040036112\n",
      "Epoch [65100/100000], Loss: 0.00026341620832681656\n",
      "Epoch [65200/100000], Loss: 0.00026285683270543814\n",
      "Epoch [65300/100000], Loss: 0.00026229830109514296\n",
      "Epoch [65400/100000], Loss: 0.00026175047969445586\n",
      "Epoch [65500/100000], Loss: 0.00026120326947420835\n",
      "Epoch [65600/100000], Loss: 0.00026065774727612734\n",
      "Epoch [65700/100000], Loss: 0.0002601135347504169\n",
      "Epoch [65800/100000], Loss: 0.000259569933405146\n",
      "Epoch [65900/100000], Loss: 0.0002590293006505817\n",
      "Epoch [66000/100000], Loss: 0.00025849006487987936\n",
      "Epoch [66100/100000], Loss: 0.0002579514402896166\n",
      "Epoch [66200/100000], Loss: 0.00025741499848663807\n",
      "Epoch [66300/100000], Loss: 0.0002568797208368778\n",
      "Epoch [66400/100000], Loss: 0.00025634633493609726\n",
      "Epoch [66500/100000], Loss: 0.0002558149571996182\n",
      "Epoch [66600/100000], Loss: 0.0002552916994318366\n",
      "Epoch [66700/100000], Loss: 0.00025477271992713213\n",
      "Epoch [66800/100000], Loss: 0.00025425446801818907\n",
      "Epoch [66900/100000], Loss: 0.0002537370310164988\n",
      "Epoch [67000/100000], Loss: 0.0002532230573706329\n",
      "Epoch [67100/100000], Loss: 0.00025270908372476697\n",
      "Epoch [67200/100000], Loss: 0.00025219720555469394\n",
      "Epoch [67300/100000], Loss: 0.00025168611318804324\n",
      "Epoch [67400/100000], Loss: 0.00025117621407844126\n",
      "Epoch [67500/100000], Loss: 0.0002506674500182271\n",
      "Epoch [67600/100000], Loss: 0.00025016063591465354\n",
      "Epoch [67700/100000], Loss: 0.00024965658667497337\n",
      "Epoch [67800/100000], Loss: 0.0002491532650310546\n",
      "Epoch [67900/100000], Loss: 0.00024865783052518964\n",
      "Epoch [68000/100000], Loss: 0.00024816449149511755\n",
      "Epoch [68100/100000], Loss: 0.0002476731897331774\n",
      "Epoch [68200/100000], Loss: 0.00024718247004784644\n",
      "Epoch [68300/100000], Loss: 0.000246693060034886\n",
      "Epoch [68400/100000], Loss: 0.00024620332987979054\n",
      "Epoch [68500/100000], Loss: 0.00024571490939706564\n",
      "Epoch [68600/100000], Loss: 0.00024522762396372855\n",
      "Epoch [68700/100000], Loss: 0.00024474284145981073\n",
      "Epoch [68800/100000], Loss: 0.0002442570112179965\n",
      "Epoch [68900/100000], Loss: 0.00024377350928261876\n",
      "Epoch [69000/100000], Loss: 0.0002432920882711187\n",
      "Epoch [69100/100000], Loss: 0.00024281349033117294\n",
      "Epoch [69200/100000], Loss: 0.00024234340526163578\n",
      "Epoch [69300/100000], Loss: 0.00024187359667848796\n",
      "Epoch [69400/100000], Loss: 0.00024140476307366043\n",
      "Epoch [69500/100000], Loss: 0.00024093656975310296\n",
      "Epoch [69600/100000], Loss: 0.00024046980252023786\n",
      "Epoch [69700/100000], Loss: 0.00024000415578484535\n",
      "Epoch [69800/100000], Loss: 0.00023953861091285944\n",
      "Epoch [69900/100000], Loss: 0.0002390742301940918\n",
      "Epoch [70000/100000], Loss: 0.0002386118285357952\n",
      "Epoch [70100/100000], Loss: 0.00023815214808564633\n",
      "Epoch [70200/100000], Loss: 0.00023769547988194972\n",
      "Epoch [70300/100000], Loss: 0.0002372394665144384\n",
      "Epoch [70400/100000], Loss: 0.00023678514116909355\n",
      "Epoch [70500/100000], Loss: 0.000236331790802069\n",
      "Epoch [70600/100000], Loss: 0.00023588094336446375\n",
      "Epoch [70700/100000], Loss: 0.00023543828865513206\n",
      "Epoch [70800/100000], Loss: 0.00023499644885305315\n",
      "Epoch [70900/100000], Loss: 0.00023455546761397272\n",
      "Epoch [71000/100000], Loss: 0.00023411688744090497\n",
      "Epoch [71100/100000], Loss: 0.00023367838002741337\n",
      "Epoch [71200/100000], Loss: 0.00023323911591432989\n",
      "Epoch [71300/100000], Loss: 0.00023280234017875046\n",
      "Epoch [71400/100000], Loss: 0.00023236619017552584\n",
      "Epoch [71500/100000], Loss: 0.0002319306368008256\n",
      "Epoch [71600/100000], Loss: 0.0002314962912350893\n",
      "Epoch [71700/100000], Loss: 0.0002310613199369982\n",
      "Epoch [71800/100000], Loss: 0.00023062915715854615\n",
      "Epoch [71900/100000], Loss: 0.0002301959175383672\n",
      "Epoch [72000/100000], Loss: 0.0002297718165209517\n",
      "Epoch [72100/100000], Loss: 0.00022934976732358336\n",
      "Epoch [72200/100000], Loss: 0.00022892847482580692\n",
      "Epoch [72300/100000], Loss: 0.00022850710956845433\n",
      "Epoch [72400/100000], Loss: 0.0002280880289617926\n",
      "Epoch [72500/100000], Loss: 0.00022766811889596283\n",
      "Epoch [72600/100000], Loss: 0.00022725199232809246\n",
      "Epoch [72700/100000], Loss: 0.00022683475981466472\n",
      "Epoch [72800/100000], Loss: 0.00022641988471150398\n",
      "Epoch [72900/100000], Loss: 0.00022600486408919096\n",
      "Epoch [73000/100000], Loss: 0.0002255902800243348\n",
      "Epoch [73100/100000], Loss: 0.0002251783589599654\n",
      "Epoch [73200/100000], Loss: 0.00022476835874840617\n",
      "Epoch [73300/100000], Loss: 0.00022435930441133678\n",
      "Epoch [73400/100000], Loss: 0.00022394947882276028\n",
      "Epoch [73500/100000], Loss: 0.00022354131215251982\n",
      "Epoch [73600/100000], Loss: 0.00022313396038953215\n",
      "Epoch [73700/100000], Loss: 0.0002227301156381145\n",
      "Epoch [73800/100000], Loss: 0.00022233458003029227\n",
      "Epoch [73900/100000], Loss: 0.00022193737095221877\n",
      "Epoch [74000/100000], Loss: 0.00022154286853037775\n",
      "Epoch [74100/100000], Loss: 0.00022114894818514585\n",
      "Epoch [74200/100000], Loss: 0.0002207556099165231\n",
      "Epoch [74300/100000], Loss: 0.000220363013795577\n",
      "Epoch [74400/100000], Loss: 0.000219970679609105\n",
      "Epoch [74500/100000], Loss: 0.00021958135766908526\n",
      "Epoch [74600/100000], Loss: 0.00021919271966908127\n",
      "Epoch [74700/100000], Loss: 0.00021880381973460317\n",
      "Epoch [74800/100000], Loss: 0.0002184164768550545\n",
      "Epoch [74900/100000], Loss: 0.00021802986157126725\n",
      "Epoch [75000/100000], Loss: 0.00021764205303043127\n",
      "Epoch [75100/100000], Loss: 0.00021725882834289223\n",
      "Epoch [75200/100000], Loss: 0.0002168816135963425\n",
      "Epoch [75300/100000], Loss: 0.00021650588314514607\n",
      "Epoch [75400/100000], Loss: 0.00021612935233861208\n",
      "Epoch [75500/100000], Loss: 0.0002157553972210735\n",
      "Epoch [75600/100000], Loss: 0.00021538211149163544\n",
      "Epoch [75700/100000], Loss: 0.00021500900038518012\n",
      "Epoch [75800/100000], Loss: 0.00021463805751409382\n",
      "Epoch [75900/100000], Loss: 0.0002142657758668065\n",
      "Epoch [76000/100000], Loss: 0.0002138950803782791\n",
      "Epoch [76100/100000], Loss: 0.00021352487965486944\n",
      "Epoch [76200/100000], Loss: 0.00021315584308467805\n",
      "Epoch [76300/100000], Loss: 0.00021278754866216332\n",
      "Epoch [76400/100000], Loss: 0.0002124216698575765\n",
      "Epoch [76500/100000], Loss: 0.00021205662051215768\n",
      "Epoch [76600/100000], Loss: 0.00021169186220504344\n",
      "Epoch [76700/100000], Loss: 0.00021132678375579417\n",
      "Epoch [76800/100000], Loss: 0.00021096577984280884\n",
      "Epoch [76900/100000], Loss: 0.0002106040483340621\n",
      "Epoch [77000/100000], Loss: 0.00021024272427894175\n",
      "Epoch [77100/100000], Loss: 0.00020988161850254983\n",
      "Epoch [77200/100000], Loss: 0.00020952582417521626\n",
      "Epoch [77300/100000], Loss: 0.0002091730566462502\n",
      "Epoch [77400/100000], Loss: 0.0002088224864564836\n",
      "Epoch [77500/100000], Loss: 0.00020847238192800432\n",
      "Epoch [77600/100000], Loss: 0.00020812114235013723\n",
      "Epoch [77700/100000], Loss: 0.00020777089230250567\n",
      "Epoch [77800/100000], Loss: 0.00020742307242471725\n",
      "Epoch [77900/100000], Loss: 0.00020707504882011563\n",
      "Epoch [78000/100000], Loss: 0.00020672963000833988\n",
      "Epoch [78100/100000], Loss: 0.0002063840947812423\n",
      "Epoch [78200/100000], Loss: 0.00020603850134648383\n",
      "Epoch [78300/100000], Loss: 0.00020569481421262026\n",
      "Epoch [78400/100000], Loss: 0.00020535050134640187\n",
      "Epoch [78500/100000], Loss: 0.00020500796381384134\n",
      "Epoch [78600/100000], Loss: 0.00020466555724851787\n",
      "Epoch [78700/100000], Loss: 0.00020432753080967814\n",
      "Epoch [78800/100000], Loss: 0.00020399423374328762\n",
      "Epoch [78900/100000], Loss: 0.00020365977252367884\n",
      "Epoch [79000/100000], Loss: 0.0002033273340202868\n",
      "Epoch [79100/100000], Loss: 0.00020299534662626684\n",
      "Epoch [79200/100000], Loss: 0.00020266379578970373\n",
      "Epoch [79300/100000], Loss: 0.00020233268151059747\n",
      "Epoch [79400/100000], Loss: 0.0002020015090238303\n",
      "Epoch [79500/100000], Loss: 0.0002016711514443159\n",
      "Epoch [79600/100000], Loss: 0.00020134197256993502\n",
      "Epoch [79700/100000], Loss: 0.0002010129246627912\n",
      "Epoch [79800/100000], Loss: 0.00020068619051016867\n",
      "Epoch [79900/100000], Loss: 0.0002003596309805289\n",
      "Epoch [80000/100000], Loss: 0.00020003478857688606\n",
      "Epoch [80100/100000], Loss: 0.00019971122674178332\n",
      "Epoch [80200/100000], Loss: 0.00019938719924539328\n",
      "Epoch [80300/100000], Loss: 0.0001990645396290347\n",
      "Epoch [80400/100000], Loss: 0.00019874121062457561\n",
      "Epoch [80500/100000], Loss: 0.0001984201226150617\n",
      "Epoch [80600/100000], Loss: 0.0001980986853595823\n",
      "Epoch [80700/100000], Loss: 0.00019777720444835722\n",
      "Epoch [80800/100000], Loss: 0.0001974578044610098\n",
      "Epoch [80900/100000], Loss: 0.00019713716756086797\n",
      "Epoch [81000/100000], Loss: 0.0001968179567484185\n",
      "Epoch [81100/100000], Loss: 0.0001965008268598467\n",
      "Epoch [81200/100000], Loss: 0.0001961898524314165\n",
      "Epoch [81300/100000], Loss: 0.00019587893621064723\n",
      "Epoch [81400/100000], Loss: 0.0001955706102307886\n",
      "Epoch [81500/100000], Loss: 0.0001952625170815736\n",
      "Epoch [81600/100000], Loss: 0.000194956868654117\n",
      "Epoch [81700/100000], Loss: 0.0001946503616636619\n",
      "Epoch [81800/100000], Loss: 0.0001943442039191723\n",
      "Epoch [81900/100000], Loss: 0.00019403916667215526\n",
      "Epoch [82000/100000], Loss: 0.000193734114873223\n",
      "Epoch [82100/100000], Loss: 0.00019342984887771308\n",
      "Epoch [82200/100000], Loss: 0.00019312543736305088\n",
      "Epoch [82300/100000], Loss: 0.00019282245193608105\n",
      "Epoch [82400/100000], Loss: 0.00019251911726314574\n",
      "Epoch [82500/100000], Loss: 0.00019221706315875053\n",
      "Epoch [82600/100000], Loss: 0.00019191481987945735\n",
      "Epoch [82700/100000], Loss: 0.00019161627278663218\n",
      "Epoch [82800/100000], Loss: 0.00019132261513732374\n",
      "Epoch [82900/100000], Loss: 0.00019103047088719904\n",
      "Epoch [83000/100000], Loss: 0.0001907391706481576\n",
      "Epoch [83100/100000], Loss: 0.00019044792861677706\n",
      "Epoch [83200/100000], Loss: 0.00019015755970031023\n",
      "Epoch [83300/100000], Loss: 0.0001898664777399972\n",
      "Epoch [83400/100000], Loss: 0.00018957712745759636\n",
      "Epoch [83500/100000], Loss: 0.00018928799545392394\n",
      "Epoch [83600/100000], Loss: 0.00018899886345025152\n",
      "Epoch [83700/100000], Loss: 0.00018871035717893392\n",
      "Epoch [83800/100000], Loss: 0.00018842252029571682\n",
      "Epoch [83900/100000], Loss: 0.00018813469796441495\n",
      "Epoch [84000/100000], Loss: 0.00018784835992846638\n",
      "Epoch [84100/100000], Loss: 0.00018756362260319293\n",
      "Epoch [84200/100000], Loss: 0.00018727942369878292\n",
      "Epoch [84300/100000], Loss: 0.00018699379870668054\n",
      "Epoch [84400/100000], Loss: 0.0001867107057478279\n",
      "Epoch [84500/100000], Loss: 0.00018642671057023108\n",
      "Epoch [84600/100000], Loss: 0.00018614479631651193\n",
      "Epoch [84700/100000], Loss: 0.00018586206715554\n",
      "Epoch [84800/100000], Loss: 0.00018558021110948175\n",
      "Epoch [84900/100000], Loss: 0.00018529835506342351\n",
      "Epoch [85000/100000], Loss: 0.0001850168191595003\n",
      "Epoch [85100/100000], Loss: 0.00018473579257261008\n",
      "Epoch [85200/100000], Loss: 0.00018445569730829448\n",
      "Epoch [85300/100000], Loss: 0.0001841752527980134\n",
      "Epoch [85400/100000], Loss: 0.00018389671458862722\n",
      "Epoch [85500/100000], Loss: 0.00018361739057581872\n",
      "Epoch [85600/100000], Loss: 0.00018334381456952542\n",
      "Epoch [85700/100000], Loss: 0.00018307229038327932\n",
      "Epoch [85800/100000], Loss: 0.00018280133372172713\n",
      "Epoch [85900/100000], Loss: 0.00018253142479807138\n",
      "Epoch [86000/100000], Loss: 0.00018226260726805776\n",
      "Epoch [86100/100000], Loss: 0.00018199329497292638\n",
      "Epoch [86200/100000], Loss: 0.00018172427371609956\n",
      "Epoch [86300/100000], Loss: 0.00018145669309888035\n",
      "Epoch [86400/100000], Loss: 0.0001811883703339845\n",
      "Epoch [86500/100000], Loss: 0.00018092035315930843\n",
      "Epoch [86600/100000], Loss: 0.00018065408221445978\n",
      "Epoch [86700/100000], Loss: 0.00018038721464108676\n",
      "Epoch [86800/100000], Loss: 0.00018012041982728988\n",
      "Epoch [86900/100000], Loss: 0.00017985518206842244\n",
      "Epoch [87000/100000], Loss: 0.00017958918760996312\n",
      "Epoch [87100/100000], Loss: 0.00017932501214090735\n",
      "Epoch [87200/100000], Loss: 0.00017906188440974802\n",
      "Epoch [87300/100000], Loss: 0.00017880462110042572\n",
      "Epoch [87400/100000], Loss: 0.00017854897305369377\n",
      "Epoch [87500/100000], Loss: 0.00017829313583206385\n",
      "Epoch [87600/100000], Loss: 0.00017803694936446846\n",
      "Epoch [87700/100000], Loss: 0.00017778205801732838\n",
      "Epoch [87800/100000], Loss: 0.00017752744315657765\n",
      "Epoch [87900/100000], Loss: 0.0001772733812686056\n",
      "Epoch [88000/100000], Loss: 0.00017701927572488785\n",
      "Epoch [88100/100000], Loss: 0.00017676479183137417\n",
      "Epoch [88200/100000], Loss: 0.00017651126836426556\n",
      "Epoch [88300/100000], Loss: 0.0001762589527061209\n",
      "Epoch [88400/100000], Loss: 0.00017600812134332955\n",
      "Epoch [88500/100000], Loss: 0.00017575705714989454\n",
      "Epoch [88600/100000], Loss: 0.00017550883057992905\n",
      "Epoch [88700/100000], Loss: 0.00017526120063848794\n",
      "Epoch [88800/100000], Loss: 0.0001750134106259793\n",
      "Epoch [88900/100000], Loss: 0.0001747668720781803\n",
      "Epoch [89000/100000], Loss: 0.0001745200133882463\n",
      "Epoch [89100/100000], Loss: 0.00017427319835405797\n",
      "Epoch [89200/100000], Loss: 0.00017402810044586658\n",
      "Epoch [89300/100000], Loss: 0.0001737823331495747\n",
      "Epoch [89400/100000], Loss: 0.00017353649309370667\n",
      "Epoch [89500/100000], Loss: 0.00017329229740425944\n",
      "Epoch [89600/100000], Loss: 0.00017304877110291272\n",
      "Epoch [89700/100000], Loss: 0.00017280403699260205\n",
      "Epoch [89800/100000], Loss: 0.00017256024875678122\n",
      "Epoch [89900/100000], Loss: 0.00017231705714948475\n",
      "Epoch [90000/100000], Loss: 0.00017207436030730605\n",
      "Epoch [90100/100000], Loss: 0.00017183204181492329\n",
      "Epoch [90200/100000], Loss: 0.00017159013077616692\n",
      "Epoch [90300/100000], Loss: 0.0001713476376608014\n",
      "Epoch [90400/100000], Loss: 0.0001711061631795019\n",
      "Epoch [90500/100000], Loss: 0.00017086573643609881\n",
      "Epoch [90600/100000], Loss: 0.00017062471306417137\n",
      "Epoch [90700/100000], Loss: 0.00017038825899362564\n",
      "Epoch [90800/100000], Loss: 0.00017015461344271898\n",
      "Epoch [90900/100000], Loss: 0.00016992120072245598\n",
      "Epoch [91000/100000], Loss: 0.0001696876424830407\n",
      "Epoch [91100/100000], Loss: 0.0001694546517683193\n",
      "Epoch [91200/100000], Loss: 0.00016922071517910808\n",
      "Epoch [91300/100000], Loss: 0.00016898788453545421\n",
      "Epoch [91400/100000], Loss: 0.00016875698929652572\n",
      "Epoch [91500/100000], Loss: 0.000168524551554583\n",
      "Epoch [91600/100000], Loss: 0.00016829332162160426\n",
      "Epoch [91700/100000], Loss: 0.0001680619752733037\n",
      "Epoch [91800/100000], Loss: 0.00016783157479949296\n",
      "Epoch [91900/100000], Loss: 0.0001676009123912081\n",
      "Epoch [92000/100000], Loss: 0.0001673697552178055\n",
      "Epoch [92100/100000], Loss: 0.0001671408535912633\n",
      "Epoch [92200/100000], Loss: 0.0001669113407842815\n",
      "Epoch [92300/100000], Loss: 0.00016668159514665604\n",
      "Epoch [92400/100000], Loss: 0.0001664543233346194\n",
      "Epoch [92500/100000], Loss: 0.00016623162082396448\n",
      "Epoch [92600/100000], Loss: 0.0001660092966631055\n",
      "Epoch [92700/100000], Loss: 0.00016578672511968762\n",
      "Epoch [92800/100000], Loss: 0.0001655646483413875\n",
      "Epoch [92900/100000], Loss: 0.00016534373571630567\n",
      "Epoch [93000/100000], Loss: 0.00016512257570866495\n",
      "Epoch [93100/100000], Loss: 0.00016490244888700545\n",
      "Epoch [93200/100000], Loss: 0.0001646821474423632\n",
      "Epoch [93300/100000], Loss: 0.00016446148219984025\n",
      "Epoch [93400/100000], Loss: 0.00016424176283180714\n",
      "Epoch [93500/100000], Loss: 0.0001640229602344334\n",
      "Epoch [93600/100000], Loss: 0.0001638039539102465\n",
      "Epoch [93700/100000], Loss: 0.00016358622815459967\n",
      "Epoch [93800/100000], Loss: 0.00016336908447556198\n",
      "Epoch [93900/100000], Loss: 0.0001631511840969324\n",
      "Epoch [94000/100000], Loss: 0.0001629364851396531\n",
      "Epoch [94100/100000], Loss: 0.00016272111679427326\n",
      "Epoch [94200/100000], Loss: 0.00016250832413788885\n",
      "Epoch [94300/100000], Loss: 0.00016229375614784658\n",
      "Epoch [94400/100000], Loss: 0.0001620806142454967\n",
      "Epoch [94500/100000], Loss: 0.00016186683205887675\n",
      "Epoch [94600/100000], Loss: 0.00016165440320037305\n",
      "Epoch [94700/100000], Loss: 0.00016144159599207342\n",
      "Epoch [94800/100000], Loss: 0.0001612287014722824\n",
      "Epoch [94900/100000], Loss: 0.00016101646178867668\n",
      "Epoch [95000/100000], Loss: 0.00016080528439488262\n",
      "Epoch [95100/100000], Loss: 0.00016059327754192054\n",
      "Epoch [95200/100000], Loss: 0.00016038198373280466\n",
      "Epoch [95300/100000], Loss: 0.00016017252346500754\n",
      "Epoch [95400/100000], Loss: 0.00015996246656868607\n",
      "Epoch [95500/100000], Loss: 0.00015975291898939759\n",
      "Epoch [95600/100000], Loss: 0.00015954370610415936\n",
      "Epoch [95700/100000], Loss: 0.00015933573013171554\n",
      "Epoch [95800/100000], Loss: 0.00015912699745967984\n",
      "Epoch [95900/100000], Loss: 0.00015891817747615278\n",
      "Epoch [96000/100000], Loss: 0.0001587111910339445\n",
      "Epoch [96100/100000], Loss: 0.00015850311319809407\n",
      "Epoch [96200/100000], Loss: 0.00015829560288693756\n",
      "Epoch [96300/100000], Loss: 0.0001580884272698313\n",
      "Epoch [96400/100000], Loss: 0.00015788222663104534\n",
      "Epoch [96500/100000], Loss: 0.00015767596778459847\n",
      "Epoch [96600/100000], Loss: 0.00015747356519568712\n",
      "Epoch [96700/100000], Loss: 0.00015727384015917778\n",
      "Epoch [96800/100000], Loss: 0.0001570729655213654\n",
      "Epoch [96900/100000], Loss: 0.00015687313862144947\n",
      "Epoch [97000/100000], Loss: 0.00015667348634451628\n",
      "Epoch [97100/100000], Loss: 0.00015647448890376836\n",
      "Epoch [97200/100000], Loss: 0.00015627508400939405\n",
      "Epoch [97300/100000], Loss: 0.0001560764794703573\n",
      "Epoch [97400/100000], Loss: 0.00015587796224281192\n",
      "Epoch [97500/100000], Loss: 0.00015568041999358684\n",
      "Epoch [97600/100000], Loss: 0.0001554831105750054\n",
      "Epoch [97700/100000], Loss: 0.00015528574294876307\n",
      "Epoch [97800/100000], Loss: 0.00015508935030084103\n",
      "Epoch [97900/100000], Loss: 0.00015489327779505402\n",
      "Epoch [98000/100000], Loss: 0.00015469698701053858\n",
      "Epoch [98100/100000], Loss: 0.00015450065257027745\n",
      "Epoch [98200/100000], Loss: 0.00015430520579684526\n",
      "Epoch [98300/100000], Loss: 0.00015411038475576788\n",
      "Epoch [98400/100000], Loss: 0.0001539141230750829\n",
      "Epoch [98500/100000], Loss: 0.0001537226198706776\n",
      "Epoch [98600/100000], Loss: 0.0001535342016723007\n",
      "Epoch [98700/100000], Loss: 0.00015334555064328015\n",
      "Epoch [98800/100000], Loss: 0.00015315607015509158\n",
      "Epoch [98900/100000], Loss: 0.00015296829224098474\n",
      "Epoch [99000/100000], Loss: 0.0001527798449387774\n",
      "Epoch [99100/100000], Loss: 0.0001525922561995685\n",
      "Epoch [99200/100000], Loss: 0.00015240468201227486\n",
      "Epoch [99300/100000], Loss: 0.00015221699140965939\n",
      "Epoch [99400/100000], Loss: 0.0001520301157142967\n",
      "Epoch [99500/100000], Loss: 0.00015184268704615533\n",
      "Epoch [99600/100000], Loss: 0.0001516557385912165\n",
      "Epoch [99700/100000], Loss: 0.00015146940131671727\n",
      "Epoch [99800/100000], Loss: 0.00015128256927710027\n",
      "Epoch [99900/100000], Loss: 0.00015109677042346448\n"
     ]
    }
   ],
   "source": [
    "# Implicitly create a new experiment\n",
    "mlflow.set_experiment(\"XOR\")\n",
    "\n",
    "epochs = 100000\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the hyperparameters\n",
    "    # Hyperparameters\n",
    "    hp = {\n",
    "        \"activation\": activation.__name__,\n",
    "        \"lr\": 0.02,\n",
    "        \"momentum\": 0.9,\n",
    "        \"epochs\": epochs,\n",
    "        \"loss_fn\": loss_fn.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "    }\n",
    "    mlflow.log_params(hp)\n",
    "    # Train the model\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item()}\")\n",
    "            mlflow.log_metric(\"loss\", f\"{loss:2f}\", step=epoch)\n",
    "\n",
    "    # Save the trained model to MLflow.\n",
    "    mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autologging\n",
    "\n",
    "That was easy! It could get tedious, however, when validation and test sets get introduced. Also, we have to manually call all the logging functions everytime we want to save more data to MLflow.\n",
    "\n",
    "Luckily, MLflow comes with `autologging`! Instead of adding all the calls yourself, simply call `mlflow.autolog` any time before `mlflow.start_run`! Make sure to checkout MLflow's guide on [\"Automatic Logging with MLflow Tracking\"](https://mlflow.org/docs/latest/tracking/autolog.html) if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs\n",
    "\n",
    "When we look at our previous attempt at solving the XOR problem, we have to admit that we were not particularly successful.\n",
    "\n",
    "Let's analyze the loss curve. In the UI, select the XOR experiment and then click on the `Chart` tab.\n",
    "\n",
    "![XOR identity chart](imgs/xor_identity_chart.png)\n",
    "It looks like the model hasn't learnt much. Let's swap out the `nn.Identity` for an `F.sigmoid` and rerun the training.\n",
    "After refreshing the page, there should now be an additional run.\n",
    "\n",
    "![XOR sigmoid chart](imgs/xor_sigmoid_chart.png)\n",
    "\n",
    "It looks like the model is finally starting to learn something after step 9000. Maybe it needs more iterations? Increase the the number of epochs to `100000`.\n",
    "\n",
    "![XOR sigmoid chart with more iterations](imgs/xor_sigmoid_chart_with_more_iterations.png)\n",
    "\n",
    "Ah! This looks much better!\n",
    "\n",
    "---\n",
    "\n",
    "There are many more features to the chart view, which we invite you to explore on your own.\n",
    "MLflow's comparison features really begin to shine when it comes to hyperparameter tuning. In the second part of this lab, you will be introduced to a state-of-the-art hyperparameter tuning package and get to play a game of _guess the hyperparameter_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-lab-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
