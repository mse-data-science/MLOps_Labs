{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Experiment Management\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "- How experiment management brings observability to ML model development\n",
    "- Workflows for using MLFlow in experiment management, including metric logging, artifact versioning, and hyperparameter optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Management with MLFLow\n",
    "\n",
    "We will be using MLflow Tracking for experiment management. The MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results.\n",
    "\n",
    "There are two important concepts:\n",
    "\n",
    "- **Runs**: Runs are executions of some piece of data science code (e.g. `python train.py`). Each run records metadata (metrics, parameters, start and end times) and as well as the artifacts produced by the code (e.g. model weights).\n",
    "- **Experiments**: An experiment groups together runs for a specific task.\n",
    "\n",
    "![MLFlow concepts](https://mlflow.org/docs/latest/_images/tracking-basics.png)\n",
    "\n",
    "### Launching the MLflow tracking server\n",
    "\n",
    "There are various deployment configurations possible for MLflow. Here we'll simply run it locally, and store everything to local files, but a production setup would usually use cloud storage for artifacts and a database for metadata.\n",
    "\n",
    "![MLflow tracking server setups](https://mlflow.org/docs/latest/assets/images/tracking-setup-overview-3d8cfd511355d9379328d69573763331.png)\n",
    "\n",
    "To start a local tracking server, run the following in a shell:\n",
    "\n",
    "```shell\n",
    "mlflow server --host 127.0.0.1 --port 8080\n",
    "```\n",
    "\n",
    "### Using the MLflow Client API\n",
    "\n",
    "The `MlflowClient` is one of the primary mechanisms that you will use when training ML models. It enables you to\n",
    "\n",
    "- create new experiments\n",
    "- start runs within experiments\n",
    "- document parameters and metrics for your runs\n",
    "- log artifacts linked to your runs\n",
    "\n",
    "First, import the `MlflowClient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `MlfLowClient` will designate local storage as the tracking server. This means that your experiments, data, models, and everything else you log to MLflow will be stored within the current working directory.\n",
    "\n",
    "To connect to a tracking server, you can set the `tracking_uri` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Default Experiment\n",
    "\n",
    "The Default Experiment is a placeholder that will be used if no explicit experiment is declared. It acts as a fallback for you to ensure that your valuable tracking data is not lost, even if you forget so explicitly create an experiment.\n",
    "\n",
    "Let's see what this default experiment looks like. We can search the available experiments using `MlflowClient.search_experiments()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = client.search_experiments()\n",
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, `search_experiments` returns a list of `Experiment` objects. `Experiment`s come an ID (`experiment_id`), a storage location for their artifacts (`artifact_location`) and a couple of time stamps - and tags. Tags allow you to attach more information to an experiment. The UI allows you to search for these tags. One \"special\" tag is `mlflow.note.content`, which you can use to attach a note to your experiment.\n",
    "\n",
    "#### Creating an experiment\n",
    "\n",
    "Creating an experiment is straightforward. In the following cell, we demonstrate how to create an experiment with additional metadata attached to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"This is an experiment for a coffee shop to forecast sales.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"coffee-forecasting\",\n",
    "    \"team\": \"stores-ml\",\n",
    "    \"project_quarter\": \"Q1-2024\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "coffee_experiment = client.create_experiment(\n",
    "    name=\"Coffee_Models\", tags=experiment_tags\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the cell above, head over to your MLflow instance. You should see a new experiment in the `Experiments` menu.\n",
    "\n",
    "![image.png](imgs/important_ui_components.png)\n",
    "\n",
    "There are a couple of UI components that are noteworthy here:\n",
    "\n",
    "![image.png](imgs/important_ui_concepts_annotated.png)\n",
    "\n",
    "As you can see, some of the tags we set previously are visible in the UI. Others are not, but they can still be searched using the search mask or the API. You can search experiments using tasks by setting the `filter_string`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_experiment = client.search_experiments(filter_string=\"tags.`project_name` = 'coffee-forecasting'\")\n",
    "coffee_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are of course better ways of accessing experiments by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_experiment = client.get_experiment_by_name(\"Coffee_Models\")\n",
    "coffee_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging to Mlflow\n",
    "\n",
    "In this section we'll be taking a closer look at the core features of MLflow Tracking:\n",
    "- creating new runs using the `start_run` context manager\n",
    "- an introduction to logging\n",
    "- the role of model signatures\n",
    "- logging a trained model\n",
    "\n",
    "#### Keeping track of training\n",
    "\n",
    "As an example, we will be forecasting coffee shop sales (a given, after the previous lab) using machine learning.\n",
    "\n",
    "For our forecasting needs, we will be using [`prophet`](https://facebook.github.io/prophet/). Prophet is a \"forecasting procedure\" developed by Meta. It is fully automated and usually a great start for any time series forecasting project. There's no need to understand the details of Prophet for the purpose of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not importing the `MlflowClient` here. Instead, we will be using the `fluent` API. The fluent API is a globally referenced state of the MLFlow tracking server. This global reference is higher-level API to perform the same actions as the `MlflowClient`.\n",
    "\n",
    "To connect to the MLflow tracking server, simply set the tracking URI as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the experiment, run name and artifact path. If you do not set a run name, MLflow will generate one for you.\n",
    "The artifact path is the path that your model will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_experiment = mlflow.set_experiment(\"Coffee_Models\")\n",
    "run_name = \"coffee_forecast_prophet\"\n",
    "artifact_path = \"coffee_prophet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these definitions out of the way, we can now start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin with some data wrangling to prepare the data for Prophet\n",
    "df = pd.read_csv(\"data/coffee_sales.csv\")\n",
    "subset = df[(df[\"product_id\"] == 32) & (df[\"store_id\"] == 8)]\n",
    "\n",
    "# For each day in transaction_date, sum the transaction_qty\n",
    "daily_sales = subset.groupby(\"transaction_date\").agg({\"transaction_qty\": \"sum\"}).reset_index()\n",
    "daily_sales.columns = [\"ds\", \"y\"]\n",
    "daily_sales[\"y\"] = daily_sales[\"y\"].astype(float)\n",
    "\n",
    "# Split the last 30 days of data into a test set\n",
    "train = daily_sales.iloc[:-30]\n",
    "test = daily_sales.iloc[-30:].reset_index()\n",
    "\n",
    "\n",
    "# Define hyperparameters for the Prophet model. Their meaning is not important. \n",
    "# We are just demonstrating how to log hyperparameters\n",
    "params = {\n",
    "    \"seasonality_mode\": \"multiplicative\",\n",
    "    \"changepoint_prior_scale\": 0.05,\n",
    "    \"seasonality_prior_scale\": 10.0,\n",
    "    \"holidays_prior_scale\": 10.0,\n",
    "    \"mcmc_samples\": 0,\n",
    "}\n",
    "\n",
    "# Create a Prophet model and fit it to the training data\n",
    "model = Prophet()\n",
    "model.fit(train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "forecast = model.predict(test)\n",
    "\n",
    "# Compare forecasted values to test set\n",
    "mape = (abs(test[\"y\"] - forecast[\"yhat\"]) / test[\"y\"]).mean()\n",
    "rmse = ((test[\"y\"] - forecast[\"yhat\"]) ** 2).mean() ** 0.5\n",
    "metrics = {\"mape\": mape, \"rmse\": rmse}\n",
    "\n",
    "# Start the MLflow run\n",
    "with mlflow.start_run(run_name=run_name, tags={\"model\": \"Prophet\"}) as run:\n",
    "    # Log the model's hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "    # Log the model's metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    # Log the model itself\n",
    "    mlflow.prophet.log_model(model, artifact_path=artifact_path, input_example=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the previous cell:\n",
    "\n",
    "1. We wrangled some data - nothing new here.\n",
    "2. We created a model using the parameters defined in `params` and fit it to the training data.\n",
    "3. We tested it on a test set and computed some metrics.\n",
    "4. This is where it gets interesting from an MLflow perspective: We created a run using the previously defined `run_name` and then logged the `params`, `metrics`, and the `model` itself to MLflow. When logging a mode, you can pass an example input. This allows MLFlow to infer the signature of your model.\n",
    "\n",
    "Note the `mlflow.prophet.log_model` function: MLflow supports a range of machine learning and deep learning frameworks (they call them [\"model flavors\"](https://mlflow.org/docs/latest/models.html#built-in-model-flavors)). If there is an obscure framework they do are not supporting, you can always log [python functions](https://mlflow.org/docs/latest/models.html#python-function-python-function) and raw files directly. Generally, you can log almost everything to MLflow and they offer dedicated functions for a range of artifacts (e.g. matplotlib `Figure`s, images, numpy data). Refer to the [MLflow docs](https://mlflow.org/docs/latest/python_api/mlflow.html) for a complete list.\n",
    "\n",
    "Your `Coffee_Models` experiment should now look something like the screenshot below.\n",
    "\n",
    "![Coffee_Models with content](imgs/Coffee_Models_with_content.png)\n",
    "\n",
    "You can click on the run to reveal detailed information about the run you logged, including the parameters, metrics, and artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with MLflow\n",
    "\n",
    "So far, we've seen a model that is relatively quick to train. Deep learning models, however, can train for days. We'll now see how MLflow can be used to monitor the training of deep models, similar to tools like tensorboard or weights and biases.\n",
    "\n",
    "As an example, let's (try) solve the [(in)famous XOR-problem](https://en.wikipedia.org/wiki/Perceptron#Universal_approximation_theorem) using a pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "%matplotlib inline\n",
    "torch.manual_seed(2)\n",
    "\n",
    "# Data\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Model\n",
    "class XOR(nn.Module):\n",
    "    def __init__(self, activation=F.sigmoid):\n",
    "        super(XOR, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "# We need room for improvement ;)    \n",
    "activation = nn.Identity()\n",
    "model = XOR(activation=activation)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, nothing changed. You declare your model and loss function, and select an optimizer.\n",
    "\n",
    "They only place that requires some changes is the training loop. Here, we are going to log the training loss with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implicitly create a new experiment\n",
    "mlflow.set_experiment(\"XOR\")\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the hyperparameters\n",
    "    # Hyperparameters\n",
    "    hp = {\n",
    "        \"activation\": activation.__class__.__name__,\n",
    "        \"lr\": 0.02,\n",
    "        \"momentum\": 0.9,\n",
    "        \"epochs\": epochs,\n",
    "        \"loss_fn\": loss_fn.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "    }\n",
    "    mlflow.log_params(hp)\n",
    "    # Train the model\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item()}\")\n",
    "            mlflow.log_metric(\"loss\", f\"{loss:2f}\", step=epoch)\n",
    "\n",
    "    # Save the trained model to MLflow.\n",
    "    mlflow.pytorch.log_model(model, \"model\", input_example=X.to(\"cpu\").numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autologging\n",
    "\n",
    "That was easy! It could get tedious, however, when validation and test sets get introduced. Also, we have to manually call all the logging functions everytime we want to save more data to MLflow.\n",
    "\n",
    "Luckily, MLflow comes with `autologging`! Instead of adding all the calls yourself, simply call `mlflow.autolog` any time before `mlflow.start_run`! Make sure to checkout MLflow's guide on [\"Automatic Logging with MLflow Tracking\"](https://mlflow.org/docs/latest/tracking/autolog.html) if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs\n",
    "\n",
    "When we look at our previous attempt at solving the XOR problem, we have to admit that we were not particularly successful.\n",
    "\n",
    "Let's analyze the loss curve. In the UI, select the XOR experiment and then click on the `Chart` tab.\n",
    "\n",
    "![XOR identity chart](imgs/xor_identity_chart.png)\n",
    "It looks like the model hasn't learnt much. Let's swap out the `nn.Identity` for an `F.sigmoid` and rerun the training.\n",
    "After refreshing the page, there should now be an additional run.\n",
    "\n",
    "![XOR sigmoid chart](imgs/xor_sigmoid_chart.png)\n",
    "\n",
    "It looks like the model is finally starting to learn something after step 9000. Maybe it needs more iterations? Increase the the number of epochs to `100000`.\n",
    "\n",
    "![XOR sigmoid chart with more iterations](imgs/xor_sigmoid_chart_with_more_iterations.png)\n",
    "\n",
    "Ah! This looks much better!\n",
    "\n",
    "---\n",
    "\n",
    "There are many more features to the chart view, which we invite you to explore on your own.\n",
    "MLflow's comparison features really begin to shine when it comes to hyperparameter tuning. In the second part of this lab, you will be introduced to a state-of-the-art hyperparameter tuning package and get to play a game of _guess the hyperparameter_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-lab-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
