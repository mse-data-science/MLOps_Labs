{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection with VAEs\n",
    "\n",
    "In the cell below, you can find the implementation of VAE. For this lab, it is not essential to understand how the model works, but you are, of course, invited to spend some pondering the code. It is based on the implementation described [here](https://hunterheidenreich.com/posts/modern-variational-autoencoder-in-pytorch/) and uses many recent PyTorch features.\n",
    "\n",
    "If you don't want to wait, you can download a (not so good) checkpoint from [here]() and skip all the training-related cells after the model definition. Make sure to set the variables below accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODEL = True  # Set to False if you don't want to train a new VAE.\n",
    "CKPT_PATH = None  # Set to path of the checkpoint you downloaded / trained previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) class.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input data.\n",
    "        hidden_dim (int): Dimensionality of the hidden layer.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    @dataclass\n",
    "    class VAEOutput:\n",
    "        \"\"\"\n",
    "        Dataclass for VAE output.\n",
    "        \n",
    "        Attributes:\n",
    "            z_dist (torch.distributions.Distribution): The distribution of the latent variable z.\n",
    "            z_sample (torch.Tensor): The sampled value of the latent variable z.\n",
    "            x_recon (torch.Tensor): The reconstructed output from the VAE.\n",
    "            loss (torch.Tensor): The overall loss of the VAE.\n",
    "            loss_recon (torch.Tensor): The reconstruction loss component of the VAE loss.\n",
    "            loss_kl (torch.Tensor): The KL divergence component of the VAE loss.\n",
    "        \"\"\"\n",
    "        z_dist: torch.distributions.Distribution\n",
    "        z_sample: torch.Tensor\n",
    "        x_recon: torch.Tensor\n",
    "        \n",
    "        loss: torch.Tensor\n",
    "        loss_recon: torch.Tensor\n",
    "        loss_kl: torch.Tensor\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "                \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 8),\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(hidden_dim // 8, 2 * latent_dim), # 2 for mean and variance.\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 8),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 8, hidden_dim // 4),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def encode(self, x, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Encodes the input data into the latent space.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            eps (float): Small value to avoid numerical instability.\n",
    "        \n",
    "        Returns:\n",
    "            torch.distributions.MultivariateNormal: Normal distribution of the encoded data.\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=-1)\n",
    "        scale = self.softplus(logvar) + eps\n",
    "        scale_tril = torch.diag_embed(scale)\n",
    "        \n",
    "        return torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)\n",
    "        \n",
    "    def reparameterize(self, dist):\n",
    "        \"\"\"\n",
    "        Reparameterizes the encoded data to sample from the latent space.\n",
    "        \n",
    "        Args:\n",
    "            dist (torch.distributions.MultivariateNormal): Normal distribution of the encoded data.\n",
    "        Returns:\n",
    "            torch.Tensor: Sampled data from the latent space.\n",
    "        \"\"\"\n",
    "        return dist.rsample()\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the data from the latent space to the original input space.\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Data in the latent space.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed data in the original input space.\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            compute_loss (bool): Whether to compute the loss or not.\n",
    "        \n",
    "        Returns:\n",
    "            VAEOutput: VAE output dataclass.\n",
    "        \"\"\"\n",
    "        dist = self.encode(x)\n",
    "        z = self.reparameterize(dist)\n",
    "        recon_x = self.decode(z)\n",
    "        \n",
    "        if not compute_loss:\n",
    "            return VAE.VAEOutput(\n",
    "                z_dist=dist,\n",
    "                z_sample=z,\n",
    "                x_recon=recon_x,\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_kl=None,\n",
    "            )\n",
    "        \n",
    "        # compute loss terms \n",
    "        loss_recon = F.binary_cross_entropy(recon_x, x + 0.5, reduction='none').sum(-1).mean()\n",
    "        std_normal = torch.distributions.MultivariateNormal(\n",
    "            torch.zeros_like(z, device=z.device),\n",
    "            scale_tril=torch.eye(z.shape[-1], device=z.device).unsqueeze(0).expand(z.shape[0], -1, -1),\n",
    "        )\n",
    "        loss_kl = torch.distributions.kl.kl_divergence(dist, std_normal).mean()\n",
    "                \n",
    "        loss = loss_recon + loss_kl\n",
    "        \n",
    "        return VAE.VAEOutput(\n",
    "            z_dist=dist,\n",
    "            z_sample=z,\n",
    "            x_recon=recon_x,\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_kl=loss_kl,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell just prepares the data. Nothing you haven't seen before, but make sure to execute it, even if you don't want to train the model. We need the data later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Lambda(lambda x: x.view(-1) - 0.5),\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform=transform,\n",
    ")\n",
    "# Download and load the test data\n",
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(input_dim=784, hidden_dim=512, latent_dim=16).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three cells define training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, prev_updates):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.view(data.size(0), -1)  # Flatten the data\n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        output = model(data)  # Forward pass\n",
    "        loss = output.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if n_upd % 100 == 0:\n",
    "            # Calculate and log gradient norms\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "            print(f'Step {n_upd:,} (N samples: {n_upd*batch_size:,}), Loss: {loss.item():.4f} (Recon: {output.loss_recon.item():.4f}, KL: {output.loss_kl.item():.4f}) Grad: {total_norm:.4f}')\n",
    "            \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    \"\"\"\n",
    "    Tests the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        cur_step (int): The current step.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    test_recon_loss = 0\n",
    "    test_kl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing'):\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)  # Flatten the data\n",
    "            \n",
    "            output = model(data, compute_loss=True)  # Forward pass\n",
    "            \n",
    "            test_loss += output.loss.item()\n",
    "            test_recon_loss += output.loss_recon.item()\n",
    "            test_kl_loss += output.loss_kl.item()\n",
    "            \n",
    "    test_loss /= len(dataloader)\n",
    "    test_recon_loss /= len(dataloader)\n",
    "    test_kl_loss /= len(dataloader)\n",
    "    print(f'====> Test set loss: {test_loss:.4f} (BCE: {test_recon_loss:.4f}, KLD: {test_kl_loss:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you set the `TRAIN_MODEL` and `CKPT_PATH` variables correctly, depending on whether you want to train a new model or not, then execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "prev_updates = 0\n",
    "if TRAIN_MODEL:\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        prev_updates = train(model, train_loader, optimizer, prev_updates)\n",
    "        test(model, test_loader)\n",
    "else:\n",
    "    model.load_state_dict(torch.load())\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'outlier_vae.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate that our model works, let's quickly validate the reconstructions by visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Select a random batch of images from the test dataset\n",
    "data, _ = next(iter(test_loader))\n",
    "data = data.to(device)\n",
    "\n",
    "# Generate reconstructions\n",
    "with torch.no_grad():\n",
    "    output = model(data, compute_loss=False)\n",
    "    reconstructions = output.x_recon.cpu().numpy()\n",
    "\n",
    "# Plot the original images and their reconstructions\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(20, 4))\n",
    "\n",
    "for i in range(10):\n",
    "    # Plot original image\n",
    "    axes[0, i].imshow(data[i].view(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot reconstructed image\n",
    "    axes[1, i].imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's shift our attention to the topic for which we are actually here: Detecting outliers.\n",
    "As mentioned in the introduction, we are trying to detect outliers by checking how well the input data can be reconstructed.\n",
    "For this, we have to set a threshold, above which we declare a sample an _outlier_. We will take care of this later, for now, just assume we have it already. Finish the implementation of the function in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier(vae, x, th):\n",
    "    \"\"\"\n",
    "    vae: The variational autoencoder.\n",
    "    x: The sample we want to compare against training the distribution.\n",
    "    th: The detection threshold.\n",
    "    \"\"\"\n",
    "    # Compute reconstruction loss for x.\n",
    "    output = vae(x, compute_loss=True)\n",
    "\n",
    "    # Compare loss against threshold.\n",
    "    if output.loss.item() > th:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've procrastinated for long enough. Let's find the threshold. Here is our approach:\n",
    "\n",
    "1. Compute the reconstruction loss for (a subset of) the training samples.\n",
    "2. Assume that x% of the data are outliers.\n",
    "3. Set the threshold to the x-th percentile.\n",
    "\n",
    "As you can see, this is not exact science. In reality, the setting of such thresholds is a long process of calibration until you have found a value that is suitable for your use case. As is often the case in statistics, it comes down to balancing \n",
    "\n",
    "Here, let's assume that 1% of all samples are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the threshold!\n",
    "threshold = 0.0\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(train_loader, desc='Computing threshold'):\n",
    "        data = data.to(device)\n",
    "        data = data.view(data.size(0), -1)  # Flatten the data\n",
    "\n",
    "        output = model(data, compute_loss=True)  # Forward pass\n",
    "        \n",
    "        # TODO: Compute the reconstruction loss.\n",
    "        loss = output.loss_recon.item()\n",
    "        losses.append(loss)\n",
    "\n",
    "# Select the 99th percentile of the loss as the threshold\n",
    "losses = torch.tensor(losses)\n",
    "threshold = torch.quantile(losses, 0.99).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we have everything to detect outliers. To test our model, let's create some distorted samples!\n",
    "For this we'll use alibi-detect, a library that provides a lot of tools for outlier detection and similar tasks. (Yes, we could have used it for the VAE as well, but where's the fun in that?)\n",
    "You can customize the distortion by changing the parameters in the call to [`apply_mask`](https://docs.seldon.io/projects/alibi-detect/en/latest/api/alibi_detect.utils.perturbation.html#alibi_detect.utils.perturbation.apply_mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from alibi_detect.utils.perturbation import apply_mask\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Index of the sample we want to perturb.\n",
    "i = 0\n",
    "\n",
    "# create masked instance\n",
    "x = test_data[i][0].reshape(1, 28, 28, 1)\n",
    "x_perturbed, _ = torch.tensor(apply_mask(\n",
    "    x,\n",
    "    mask_size=(8, 8),\n",
    "    n_masks=1,\n",
    "    channels=[0],\n",
    "    mask_type=\"normal\",\n",
    "    noise_distr=(0, 0.5),\n",
    "    clip_rng=(0, 1),\n",
    ")).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_perturbed.reshape(28, 28))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these samples, let's test the `detect_outliers` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of target should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdetect_outlier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_perturbed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m, in \u001b[0;36mdetect_outlier\u001b[0;34m(vae, x, th)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mvae: The variational autoencoder.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mx: The sample we want to compare against training the distribution.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mth: The detection threshold.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute reconstruction loss for x.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compare loss against threshold.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m th:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 134\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x, compute_loss)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VAE\u001b[38;5;241m.\u001b[39mVAEOutput(\n\u001b[1;32m    125\u001b[0m         z_dist\u001b[38;5;241m=\u001b[39mdist,\n\u001b[1;32m    126\u001b[0m         z_sample\u001b[38;5;241m=\u001b[39mz,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         loss_kl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m     )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# compute loss terms \u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m loss_recon \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    135\u001b[0m std_normal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMultivariateNormal(\n\u001b[1;32m    136\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros_like(z, device\u001b[38;5;241m=\u001b[39mz\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    137\u001b[0m     scale_tril\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39meye(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mz\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m loss_kl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mkl\u001b[38;5;241m.\u001b[39mkl_divergence(dist, std_normal)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of target should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "detect_outlier(model, x_perturbed.flatten(), threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
